WEBVTT

1
00:00:00.000 --> 00:00:01.530
以前のビデオでは、

2
00:00:01.530 --> 00:00:04.227
あなたは、ロジスティック回帰モデルを見ました。

3
00:00:04.227 --> 00:00:07.526
ロジスティック回帰モデルのWとBのパラメータを訓練するために、

4
00:00:07.526 --> 00:00:10.570
あなたはコスト関数を定義する必要があります。

5
00:00:10.570 --> 00:00:14.430
あなたはロジスティック回帰を訓練するために使用することができ、コスト関数を見てみましょう。

6
00:00:14.430 --> 00:00:18.195
要約すると、これは我々が前のスライドから探していたものです。

7
00:00:18.195 --> 00:00:20.792
だからあなたの出力yハットは、ワットのシグモイドです

8
00:00:20.792 --> 00:00:24.690
Zのシグモイドがここで定義されているように、XプラスBの転置。

9
00:00:24.690 --> 00:00:27.600
ですからが与えられているモデルのパラメータを学習します

10
00:00:27.600 --> 00:00:31.200
m個の訓練例とそのトレーニングセット

11
00:00:31.200 --> 00:00:34.060
あなたはWパラメータを見つけたいという自然なようです

12
00:00:34.060 --> 00:00:37.781
そして、Bように、少なくとも訓練セットで、出力はあなたが持っています。

13
00:00:37.781 --> 00:00:40.225
あなたがトレーニングセットに持って予測、

14
00:00:40.225 --> 00:00:43.260
それは近くになることをどの我々は唯一のようにYハット（i）を書きます

15
00:00:43.260 --> 00:00:47.720
グランド真実はあなたがトレーニングセットになったことをY_Iラベル。

16
00:00:47.720 --> 00:00:52.110
だから、上式のためにもう少し詳細に投げるために、

17
00:00:52.110 --> 00:00:56.205
我々はのための一番上に定義したように、Y-帽子があると言っていました

18
00:00:56.205 --> 00:01:00.930
トレーニングの例xとコースの各トレーニング例えば、

19
00:01:00.930 --> 00:01:03.240
我々は、これらの上付き文字とを使用しています

20
00:01:03.240 --> 00:01:07.710
括弧のインデックスへと例を区別するために丸括弧。

21
00:01:07.710 --> 00:01:12.870
Yハット（i）は学習サンプル（I）上のあなたの予測がために起こっています

22
00:01:12.870 --> 00:01:18.835
シグモイド関数を取り、W転置Xにそれを適用することにより得られます、

23
00:01:18.835 --> 00:01:25.905
（I）の訓練例プラスV、あなたはまた、Z（i）を定義することができ、入力としては、以下の通りです。

24
00:01:25.905 --> 00:01:30.110
Z（i）はWの転置X（I）に加えて、Bに等しいです。

25
00:01:30.110 --> 00:01:31.350
したがって、このコースを通して、

26
00:01:31.350 --> 00:01:33.966
我々は、この表記規則を使用するつもりです、

27
00:01:33.966 --> 00:01:41.605
上付き括弧ことを私はデータを参照します。

28
00:01:41.605 --> 00:01:47.615
XまたはYまたはZまたはi番目のトレーニング例に関連した何か他のもの、

29
00:01:47.615 --> 00:01:50.885
i番目の例に関連付けられています。

30
00:01:50.885 --> 00:01:54.840
これは、カッコ内の上付き文字、私が何を意味するかです。

31
00:01:54.840 --> 00:01:57.630
それでは、どのような損失関数やエラーを見てみましょう

32
00:01:57.630 --> 00:02:01.315
我々はアルゴリズムがやっているどれだけ測定するために使用できる機能。

33
00:02:01.315 --> 00:02:06.015
あなたができることの一つは、ときに、アルゴリズムの出力損失を定義しています

34
00:02:06.015 --> 00:02:12.320
YとしてY-帽子と真のラベル二乗誤差多分二乗誤差または半分であることを。

35
00:02:12.320 --> 00:02:14.975
それはあなたがこれを行うことができることが判明し、

36
00:02:14.975 --> 00:02:17.670
しかし、ロジスティック回帰の人々は、通常は行いません。

37
00:02:17.670 --> 00:02:21.000
あなたはパラメータを学ぶために来て、この理由は、

38
00:02:21.000 --> 00:02:25.682
あなたは私たちがおよそ後で話を最適化問題は非凸になっていることを見つけます。

39
00:02:25.682 --> 00:02:30.105
だから、複数の局所最適解と最適化問題で終わります。

40
00:02:30.105 --> 00:02:33.285
だから、勾配降下は、大域的最適を見つけることができません。

41
00:02:33.285 --> 00:02:35.580
あなたはコメントの最後のカップルを理解していなかった場合。

42
00:02:35.580 --> 00:02:38.320
それについては心配しないでください、私たちは、後でビデオでそれを取得します。

43
00:02:38.320 --> 00:02:40.990
しかし奪う直感ということです

44
00:02:40.990 --> 00:02:44.620
損失関数と呼ばれるこの関数Lは、あなたがよ関数であります

45
00:02:44.620 --> 00:02:51.265
真のラベルをyであるとき、私たちの出力yハットがどのように良い測定するために定義する必要があります。

46
00:02:51.265 --> 00:02:54.345
二乗誤差がように思えるとして、それは合理的な選択であるかもしれません

47
00:02:54.345 --> 00:02:58.160
それは勾配降下がうまく動作しない可能ということを除いて。

48
00:02:58.160 --> 00:03:00.500
だから、ロジスティック回帰では、我々は実際に定義します

49
00:03:00.500 --> 00:03:05.695
乗誤差と同様の役割を果たしている別の損失関数、

50
00:03:05.695 --> 00:03:08.910
それは私たちにある最適化問題を与えます

51
00:03:08.910 --> 00:03:13.530
凸ので、我々は、後でビデオで表示されますが、最適化がはるかに容易になります。

52
00:03:13.530 --> 00:03:17.310
だから、私たちはロジスティック回帰に使用することは、実際にあります

53
00:03:17.310 --> 00:03:21.795
私はちょうどここまでのようだ、次の損失関数、

54
00:03:21.795 --> 00:03:31.740
負のyログのy帽子プラス1行は、yのログを持っています

55
00:03:31.740 --> 00:03:34.600
一行は、Y-帽子です。

56
00:03:34.600 --> 00:03:38.785
ここでは、この損失関数は理にかなっている理由のためのいくつかの直感です。

57
00:03:38.785 --> 00:03:41.285
私たちが使用している場合ことを覚えておいてください

58
00:03:41.285 --> 00:03:45.820
二乗誤差は、あなたは二乗誤差を可能な限り小さくすることにしたいです。

59
00:03:45.820 --> 00:03:48.680
そして、このロジスティック回帰損失関数と、

60
00:03:48.680 --> 00:03:51.495
我々はまた、これは可能な限り小さくしたいと思います。

61
00:03:51.495 --> 00:03:53.508
これは理にかなっている理由を理解するためには、

62
00:03:53.508 --> 00:03:55.260
2例を見てみましょう。

63
00:03:55.260 --> 00:03:56.570
最初のケースで、

64
00:03:56.570 --> 00:03:59.430
のは、Yが、その後1損失に等しいとしましょう

65
00:03:59.430 --> 00:04:05.415
関数Yハットコンマyが、私たちは、この負の符号を書き込むための正義です。

66
00:04:05.415 --> 00:04:08.735
したがって、この負の対数のyハット。

67
00:04:08.735 --> 00:04:10.770
yが1に等しい場合。 yが等しい場合ので、

68
00:04:10.770 --> 00:04:14.070
一つは、第2の用語1マイナスYはゼロに等しいです。

69
00:04:14.070 --> 00:04:19.880
yはあなたが負の対数のy帽子はできるだけ大きなになりたい1に等しいのであれば、これは言います。

70
00:04:19.880 --> 00:04:26.040
だから、あなたが大きくなるように、Y-帽子を記録したい意味します

71
00:04:26.040 --> 00:04:32.935
できるだけ大きいこと、それはあなたがYハットが大になりたい意味します。

72
00:04:32.935 --> 00:04:35.170
Y-帽子があるので、しかし、あなたは、知っています

73
00:04:35.170 --> 00:04:38.440
シグモイド関数は、それが1より大きくなることはありません。

74
00:04:38.440 --> 00:04:41.850
これは、yが1あなたがに等しい場合と言っています

75
00:04:41.850 --> 00:04:44.050
Yハットは、できるだけ大きなになりたいです。

76
00:04:44.050 --> 00:04:48.220
しかし、それは今まであなたがYハットは同様に1に近いものにしたいと言って1より大きくすることはできません。

77
00:04:48.220 --> 00:04:50.740
yがゼロに等しい場合、他の場合です。

78
00:04:50.740 --> 00:04:55.375
yがゼロに等しい場合、損失関数におけるこの最初の項はゼロに等しいので

79
00:04:55.375 --> 00:05:01.290
Yゼロと第2項は損失関数を定義します。

80
00:05:01.290 --> 00:05:07.210
だから、損失は負の対数1マイナスYハットになります。

81
00:05:07.210 --> 00:05:11.480
だから、あなたの学習手順で使用すると、損失関数を小さくしようとすると、

82
00:05:11.480 --> 00:05:19.450
これが意味することは、あなたが大きくなるように1つのマイナスのy帽子を記録したいということです。

83
00:05:19.450 --> 00:05:22.050
そして、それが負の符号だからと

84
00:05:22.050 --> 00:05:24.660
その後、理由の同様の作品を通じて、あなたは結論付けることができます

85
00:05:24.660 --> 00:05:30.870
この損失関数は、可能な限りのyハットのように小さくしようとしています。

86
00:05:30.870 --> 00:05:34.320
そして再びY-帽子は0と1の間でなければならないので。

87
00:05:34.320 --> 00:05:38.155
これは、もしyがゼロに等しいことを言っています

88
00:05:38.155 --> 00:05:43.790
あなたの損失関数は、限りなくゼロに近いY-帽子を作るためにパラメータをプッシュします。

89
00:05:43.790 --> 00:05:48.305
さて、もしyが等しいことをRafidahの効果と機能がたくさんあります

90
00:05:48.305 --> 00:05:52.950
1に、我々は、Y-帽子が大作ってみると、Yがゼロに等しい場合、我々は、Y-帽子を小さくしてみてください。

91
00:05:52.950 --> 00:05:55.150
私達はちょうど緑にここに与えました

92
00:05:55.150 --> 00:05:59.920
この損失関数のためにやや非公式の正当化が提供されます

93
00:05:59.920 --> 00:06:03.970
オプションのビデオは、後のために、より正式な正当性を与えるために

94
00:06:03.970 --> 00:06:08.500
なぜロジスティック回帰では、我々は、この特定の形式で損失関数を使用したいです。

95
00:06:08.500 --> 00:06:13.630
最後に、損失関数は、単一の訓練例に対して定義されました。

96
00:06:13.630 --> 00:06:16.760
それはあなたが単一のトレーニング例にやっているどれだけ測定します。

97
00:06:16.760 --> 00:06:21.148
私は今、コスト関数と呼ばれるものを定義するつもりです

98
00:06:21.148 --> 00:06:24.690
これはあなたが全体のトレーニングセットをやっているどれだけ測定します。

99
00:06:24.690 --> 00:06:28.660
に適用されるので、コスト関数J

100
00:06:28.660 --> 00:06:33.130
あなたのパラメータWとBは、のいずれかで平均になるだろう

101
00:06:33.130 --> 00:06:43.270
損失関数の和のmは訓練例及びターンの各々に適用しました。

102
00:06:43.270 --> 00:06:45.435
ここでY-帽子はもちろんですが

103
00:06:45.435 --> 00:06:49.570
あなたが知っている使用してロジスティック回帰アルゴリズムによる予測出力、

104
00:06:49.570 --> 00:06:52.430
W及びB.パラメータの特定のセット

105
00:06:52.430 --> 00:06:54.480
そしてこれだけ、これを拡大します

106
00:06:54.480 --> 00:06:58.010
これは、以上のマイナス1に等しく、

107
00:06:58.010 --> 00:07:03.550
私からメートルの合計は、損失関数の定義のmまで1に等しいです。

108
00:07:03.550 --> 00:07:07.530
だから、これはY-帽子をログのy（i）は

109
00:07:07.530 --> 00:07:14.530
（I）を加えた一行は、Y（i）1種線はyハット（I）でログです。

110
00:07:14.530 --> 00:07:17.880
私はここに角括弧を置くことができると思います。

111
00:07:17.880 --> 00:07:20.945
だから、マイナス記号は、他のすべての外にあります。

112
00:07:20.945 --> 00:07:23.665
だから、私が使用するつもり用語はということです

113
00:07:23.665 --> 00:07:29.120
損失関数はそうのように1つだけの訓練例に適用されます。

114
00:07:29.120 --> 00:07:33.010
そして、コスト関数は、あなたのパラメータの費用です。

115
00:07:33.010 --> 00:07:36.115
だからあなたのロジスティック回帰モデルのトレーニングでは、

116
00:07:36.115 --> 00:07:38.980
我々は、パラメータがWとB見つけようとしていること

117
00:07:38.980 --> 00:07:43.475
一番下に書かれた機械Jの全体的なコストを最小限に抑えることができます。

118
00:07:43.475 --> 00:07:48.040
だから、あなたはただ、ロジスティック回帰アルゴリズムのためのセットアップを見てきました

119
00:07:48.040 --> 00:07:50.770
訓練例とのための損失関数

120
00:07:50.770 --> 00:07:54.190
あなたのアルゴリズムのパラメータのための全体的なコスト関数。

121
00:07:54.190 --> 00:07:59.485
これは、ロジスティック回帰は非常に非常に小さなニューラルネットワークとみなすことができることが判明しました。

122
00:07:59.485 --> 00:08:01.905
次のビデオでは、我々はあなたが始めることができることをオーバー行きますよ

123
00:08:01.905 --> 00:08:04.965
ニューラルネットワークは何をすべきかについての直感を得ます。

124
00:08:04.965 --> 00:08:08.230
だからのは、どのようにについて次のビデオに行きましょう

125
00:08:08.230 --> 00:08:11.630
非常に小さなニューラルネットワークとしてロジスティック回帰を表示します。

126
00:06:03.960 --> 00:06:07.770
Mの訓練の上に小さなループです

127
00:06:05.490 --> 00:06:10.919
実施例および第2のforループであります

128
00:06:07.770 --> 00:06:13.139
こっちにすべての機能をループのために

129
00:06:10.919 --> 00:06:15.930
右この例のように、我々は2つだけを持っていました

130
00:06:13.139 --> 00:06:17.879
機能nが2であり、Xに等しい2になるように

131
00:06:15.930 --> 00:06:21.000
あなたはより多くの機能を持っている場合は2に等しいが、

132
00:06:17.879 --> 00:06:23.099
あなたはDW 1 DW 2を書き​​終わると、

133
00:06:21.000 --> 00:06:25.979
あなたはDW vについて同様の計算を持っています

134
00:06:23.099 --> 00:06:29.009
ようにDWまでのnだからのように思えます

135
00:06:25.979 --> 00:06:31.279
以上のループのために持っている必要があります

136
00:06:29.009 --> 00:06:33.199
n個すべての機能を超える機能

137
00:06:31.279 --> 00:06:36.049
あなたは深い学習を実施しているとき

138
00:06:33.199 --> 00:06:38.419
あなたが持つ明示的なことを見つけるアルゴリズム

139
00:06:36.049 --> 00:06:41.839
あなたのコード内のループのためのあなたを作ります

140
00:06:38.419 --> 00:06:44.149
このアルゴリズムは、あまり効率を実行し、そうで

141
00:06:41.839 --> 00:06:46.669
深い学習誤差はに移動します

142
00:06:44.149 --> 00:06:48.649
どんどん大きくデータセットとそうであること

143
00:06:46.669 --> 00:06:50.779
あなたのアルゴリズムを実装することができ

144
00:06:48.649 --> 00:06:52.969
ループの明示的な使用せずです

145
00:06:50.779 --> 00:06:55.129
本当に重要とするのに役立ちます

146
00:06:52.969 --> 00:06:56.719
それそんなに大きなデータセットのスケール

147
00:06:55.129 --> 00:06:58.129
のが設定されていることが判明

148
00:06:56.719 --> 00:07:01.159
ベクトル化と呼ばれる技術

149
00:06:58.129 --> 00:07:03.559
あなたが取り除くことを可能にする技術

150
00:07:01.159 --> 00:07:06.169
あなたのコードIにおけるこれらの明示的な完全なループ

151
00:07:03.559 --> 00:07:08.199
前深い学習時代に考えます

152
00:07:06.169 --> 00:07:11.239
それは深い学習の台頭の前です

153
00:07:08.199 --> 00:07:13.159
ベクトル化はあなたを持ってよかったです

154
00:07:11.239 --> 00:07:15.589
時には、車両の速度を上げるためにそれを行うことができ

155
00:07:13.159 --> 00:07:17.749
そして時にはないが、深いですで

156
00:07:15.589 --> 00:07:20.029
ある学習時代のベクトル化

157
00:07:17.749 --> 00:07:22.699
このようなforループを取り除くと

158
00:07:20.029 --> 00:07:25.039
このように非常に重要になってきました

159
00:07:22.699 --> 00:07:26.989
私たちは、より多くのトレーニングをしているので、

160
00:07:25.039 --> 00:07:29.239
非常に大規模なデータセットとそうあなたは本当に

161
00:07:26.989 --> 00:07:31.209
ので、非常に効率的であるためにあなたのコードが必要です

162
00:07:29.239 --> 00:07:34.219
次のいくつかの動画では、我々は話よ

163
00:07:31.209 --> 00:07:37.339
どのようにすべてを実装するベクトル化し、

164
00:07:34.219 --> 00:07:40.879
この1つでもフルを使用せず

165
00:07:37.339 --> 00:07:43.069
ループ私はあなたが意味を持っている願っています。このように、

166
00:07:40.879 --> 00:07:44.299
ロジスティック回帰を暗示する方法の

167
00:07:43.069 --> 00:07:46.339
ロジスティックまたは勾配降下

168
00:07:44.299 --> 00:07:47.959
物事の回帰が鮮明になります

169
00:07:46.339 --> 00:07:50.299
あなたは、プログラムの演習を実施したときに

170
00:07:47.959 --> 00:07:51.829
しかし、前には、実際にプログラムを実行します

171
00:07:50.299 --> 00:07:54.079
程度の運動ましょう最初の話

172
00:07:51.829 --> 00:07:56.419
ベクトルその後、あなたが実装することができるように

173
00:07:54.079 --> 00:07:58.369
この全体のことは、単一の実装します

174
00:07:56.419 --> 00:08:01.479
勾配降下の反復なし

175
00:07:58.369 --> 00:08:01.479
任意の秋のニュースを使用して
