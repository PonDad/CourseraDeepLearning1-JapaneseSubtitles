WEBVTT

1
00:00:00.000 --> 00:00:03.689
私はあなたのいくつかの絵を描く見ます

2
00:00:01.770 --> 00:00:05.759
私たちは話しましょう。このビデオでは、ニューラルネットワーク

3
00:00:03.689 --> 00:00:07.919
まさにこれらの写真手段について

4
00:00:05.759 --> 00:00:09.530
言い換えればactlyなけなしのものを

5
00:00:07.919 --> 00:00:12.660
ニューラルネットワークは、上の描画されています

6
00:00:09.530 --> 00:00:14.400
表現し、我々は集中で始まります

7
00:00:12.660 --> 00:00:16.470
ニューラルネットワークの場合に

8
00:00:14.400 --> 00:00:18.990
どのような単一の隠れ層と呼ばれています

9
00:00:16.470 --> 00:00:20.939
ここでは、ニューラルネットみましょうの画像です

10
00:00:18.990 --> 00:00:24.210
これらの写真の異なる部分を与えます

11
00:00:20.939 --> 00:00:27.420
いくつかの名前は、私たちは入力×1を備えてい

12
00:00:24.210 --> 00:00:30.390
X2 X3が縦にSTATA、これはあります

13
00:00:27.420 --> 00:00:33.030
神経回路の入力層と呼ばれます

14
00:00:30.390 --> 00:00:34.649
ネットワークので、多分驚くことではないが、この

15
00:00:33.030 --> 00:00:37.100
神経回路への入力が含まれています

16
00:00:34.649 --> 00:00:40.440
ネットワークは、その後の別の層があります

17
00:00:37.100 --> 00:00:42.480
円、これは隠されたと呼ばれています

18
00:00:40.440 --> 00:00:44.340
層が、私は来るニューラルネットワーク

19
00:00:42.480 --> 00:00:46.739
バック秒でどのような言葉を言うします

20
00:00:44.340 --> 00:00:49.260
隠された手段が、最終層はここにあります

21
00:00:46.739 --> 00:00:52.379
この場合、ただ一つのノートによって形成され、

22
00:00:49.260 --> 00:00:54.090
この単音層と呼ばれます

23
00:00:52.379 --> 00:00:57.210
出力層とする責任があります

24
00:00:54.090 --> 00:00:59.250
予測値Yの帽子を生成します

25
00:00:57.210 --> 00:01:01.050
あなたが訓練ニューラルネットワーク

26
00:00:59.250 --> 00:01:03.750
トレーニングセットを教師あり学習

27
00:01:01.050 --> 00:01:06.210
入力Xの値にも含まれてい

28
00:01:03.750 --> 00:01:08.549
ターゲットは、用語ようにYを出力するように

29
00:01:06.210 --> 00:01:10.950
中間層は、中にいるという事実を指し、

30
00:01:08.549 --> 00:01:12.780
訓練は、これらのための真の値を設定します

31
00:01:10.950 --> 00:01:14.610
真ん中のノードが観測されていません

32
00:01:12.780 --> 00:01:16.049
他の人があなたは彼らがどうあるべきかが表示されません

33
00:01:14.610 --> 00:01:17.369
トレーニングセットにあなたは何を参照します

34
00:01:16.049 --> 00:01:19.170
入力は、あなたがどのような出力が表示されます

35
00:01:17.369 --> 00:01:21.299
隠された物事のためにする必要があります

36
00:01:19.170 --> 00:01:24.090
層は、トレーニングセットで見ていません

37
00:01:21.299 --> 00:01:25.950
そのため一種の隠された名前を説明

38
00:01:24.090 --> 00:01:27.720
層は、ちょうどあなたがそれを見ることはありません意味します

39
00:01:25.950 --> 00:01:29.850
トレーニングセットはのは、ご紹介しましょうもう少し

40
00:01:27.720 --> 00:01:33.030
以前に私たちがしたのに対し、表記

41
00:01:29.850 --> 00:01:36.390
入力を示すために、ベクトルXを使用して

42
00:01:33.030 --> 00:01:38.720
以下のための代替表記を特徴と

43
00:01:36.390 --> 00:01:41.729
入力機能の値になります

44
00:01:38.720 --> 00:01:44.579
上付き四角ブラケット0と

45
00:01:41.729 --> 00:01:47.790
用語aはまた、アクティベーションの略で、

46
00:01:44.579 --> 00:01:49.829
それは別の値を参照します

47
00:01:47.790 --> 00:01:52.619
ニューラルネットワークの層が渡しています

48
00:01:49.829 --> 00:01:55.229
入力ので、後続の層へのオン

49
00:01:52.619 --> 00:01:56.820
層を値Xに通過します

50
00:01:55.229 --> 00:01:59.850
我々はそれを呼び出すするつもりだので、隠れ層

51
00:01:56.820 --> 00:02:02.700
情報層aのアクティベーションをコーディング

52
00:01:59.850 --> 00:02:05.189
次の層に隠さ上付き文字0

53
00:02:02.700 --> 00:02:07.290
層は、順番にいくつかのセットを生成します

54
00:02:05.189 --> 00:02:10.319
私のように書くつもりだアクティベーション

55
00:02:07.290 --> 00:02:13.220
その上付き正方形のブラケット1

56
00:02:10.319 --> 00:02:15.800
この最初のユニットや、この個人の難問

57
00:02:13.220 --> 00:02:18.980
我々は値上付き文字を生成します

58
00:02:15.800 --> 00:02:21.530
角括弧1添字1この第二の

59
00:02:18.980 --> 00:02:23.060
我々が今値を生成注意

60
00:02:21.530 --> 00:02:25.910
添字2というように

61
00:02:23.060 --> 00:02:29.480
そしてその上付き括弧1

62
00:02:25.910 --> 00:02:31.130
これは、4次元のベクトルである場合、または

63
00:02:29.480 --> 00:02:34.040
あなたはそれが、私たちは美しい与えPythonで欲しいです

64
00:02:31.130 --> 00:02:35.930
つのマトリックス又は4列ベクトルによって

65
00:02:34.040 --> 00:02:37.850
これは次のようになりますと、それは4です

66
00:02:35.930 --> 00:02:41.540
なぜなら、我々が持っているこの場合の寸法

67
00:02:37.850 --> 00:02:44.120
4つのノードまたは4台または4つの隠れ

68
00:02:41.540 --> 00:02:45.950
そして、最終的にはこの隠された層のユニット

69
00:02:44.120 --> 00:02:48.410
出力層は、いくつかを生成します。

70
00:02:45.950 --> 00:02:51.980
ただ実数である値2

71
00:02:48.410 --> 00:02:54.410
そのためYハットを取るために起こっています

72
00:02:51.980 --> 00:02:56.540
2の値は、これはどのように似ています

73
00:02:54.410 --> 00:02:59.720
ロジスティック回帰では、我々はYの帽子を持っています

74
00:02:56.540 --> 00:03:02.480
Aとロジスティック回帰政権に等しいです

75
00:02:59.720 --> 00:03:03.950
我々は唯一の私たちは、1つの出力層を持っていました

76
00:03:02.480 --> 00:03:06.140
上付きの正方形を使用していませんでした

77
00:03:03.950 --> 00:03:07.880
ブラケットが、ニューラルネットワークせずに私たちがしています

78
00:03:06.140 --> 00:03:10.010
今、この上付き文字の正方形を使用する予定

79
00:03:07.880 --> 00:03:12.760
明示的に示すためのブラケット

80
00:03:10.010 --> 00:03:15.410
それは約面白いから来たレイヤ

81
00:03:12.760 --> 00:03:17.269
あなたのネットワークの表記規則

82
00:03:15.410 --> 00:03:19.850
このネットワークは、あなたが見ているということです

83
00:03:17.269 --> 00:03:22.910
ここにあなたの上の層に呼び出されました

84
00:03:19.850 --> 00:03:25.010
ネットワークとその理由は、ときに我々です

85
00:03:22.910 --> 00:03:27.200
お使いのネット​​ワークでレイヤをカウントし、我々にはありません

86
00:03:25.010 --> 00:03:29.840
隠されたように、入力層を数えます

87
00:03:27.200 --> 00:03:32.150
層は、層1であり、出力層であります

88
00:03:29.840 --> 00:03:34.580
別の表記規則に層

89
00:03:32.150 --> 00:03:36.470
我々はそう入力層層0を呼んでいます

90
00:03:34.580 --> 00:03:38.420
技術的には多分3つの層があります

91
00:03:36.470 --> 00:03:39.620
このビューのネットワークであれば理由

92
00:03:38.420 --> 00:03:42.260
入力は隠れ層と層

93
00:03:39.620 --> 00:03:44.510
オープン層が、従来のユーザーであれば

94
00:03:42.260 --> 00:03:46.310
あなたの中にそのL上の研究論文

95
00:03:44.510 --> 00:03:48.470
もちろん、あなたは、人々がこれを参照参照します

96
00:03:46.310 --> 00:03:50.540
2層として、特定の神経回路網

97
00:03:48.470 --> 00:03:52.400
ニューラルネットワーク我々は中カウントされませんので、

98
00:03:50.540 --> 00:03:54.230
公式の層のように同一直線上

99
00:03:52.400 --> 00:03:56.330
我々が得られます最終的には何か

100
00:03:54.230 --> 00:03:58.580
後でである中間層と

101
00:03:56.330 --> 00:04:00.590
出力層は、パラメータを持つことになります

102
00:03:58.580 --> 00:04:02.360
隠れ層ので、それに関連付けられました

103
00:04:00.590 --> 00:04:05.930
自分に関連付けられています

104
00:04:02.360 --> 00:04:08.030
WとBと書くつもりパラメータ

105
00:04:05.930 --> 00:04:10.040
示すために、上付きの正方形ブラケット1

106
00:04:08.030 --> 00:04:12.140
これらは、関連するパラメータであることを

107
00:04:10.040 --> 00:04:15.140
隠れ層と層1と我々はよ

108
00:04:12.140 --> 00:04:18.979
Wは3で戦争になることを後で見ます

109
00:04:15.140 --> 00:04:20.780
行列とBは4 1によってベクターであろう

110
00:04:18.979 --> 00:04:22.820
最初の座標この例

111
00:04:20.780 --> 00:04:24.490
4、我々は4を持っているという事実から来ています

112
00:04:22.820 --> 00:04:26.590
ノードまたは4つの隠れユニット

113
00:04:24.490 --> 00:04:28.870
そこと3があるという事実から来ています

114
00:04:26.590 --> 00:04:30.580
私たちは話しましょう3つの入力機能を持っています

115
00:04:28.870 --> 00:04:32.470
後でこれらの寸法について

116
00:04:30.580 --> 00:04:34.810
行列は、それがでより多くの意味をなさないかもしれません

117
00:04:32.470 --> 00:04:36.729
その時が、同様にオパールで

118
00:04:34.810 --> 00:04:39.819
また、それに関連するとして、層

119
00:04:36.729 --> 00:04:42.520
上付きかっこパラメータw

120
00:04:39.819 --> 00:04:44.199
2およびB上付き四角ブラケット2及び

121
00:04:42.520 --> 00:04:46.630
これらの寸法の面であります

122
00:04:44.199 --> 00:04:48.759
4つずつ1つずつと、この1

123
00:04:46.630 --> 00:04:50.710
中間層が持っているので、四つんばいによります

124
00:04:48.759 --> 00:04:52.780
アルファ層が持つ4つの隠れユニット

125
00:04:50.710 --> 00:04:54.490
我々はオーバー行くよbergame上だけで1つのユニット

126
00:04:52.780 --> 00:04:57.220
これらの行列の大きさと

127
00:04:54.490 --> 00:04:59.470
あなただけしましたので、後でビデオでベクトル

128
00:04:57.220 --> 00:05:01.509
見てどのような2層のニューラルネットワーク

129
00:04:59.470 --> 00:05:04.180
それは、ニューラルネットワークであるように見えます

130
00:05:01.509 --> 00:05:06.340
次のビデオでは1つの隠れ層はしてみましょう

131
00:05:04.180 --> 00:05:09.009
まさにこの新しい深く行きます

132
00:05:06.340 --> 00:05:11.229
ネットワークは、それがどのようにこれをある計算されます

133
00:05:09.009 --> 00:05:15.599
ニューラルネットワークの入力Xとすべて行きます

134
00:05:11.229 --> 00:05:15.599
この出力Yの帽子を計算する方法
