WEBVTT

1
00:00:00.000 --> 00:00:03.180
最後のビデオでは、予測を計算する方法を説明しました

2
00:00:03.180 --> 00:00:06.315
ニューラルネットワーク上の単一のトレーニング例を与えられました。

3
00:00:06.315 --> 00:00:11.100
このビデオでは、複数の学習例渡ってベクトル化する方法を参照してください。

4
00:00:11.100 --> 00:00:14.685
そして結果は、あなたがロジスティック回帰のために見たものと非常によく似ています

5
00:00:14.685 --> 00:00:19.016
これにより行列の異なる列で異なるトレーニング事例を積み重ね、

6
00:00:19.016 --> 00:00:22.830
あなたは前のビデオから持っていたの方程式を取ることができるでしょう、

7
00:00:22.830 --> 00:00:24.765
そして、非常に小さな変更を加え、

8
00:00:24.765 --> 00:00:29.340
ニューラルネットワークは、すべての例の出力を計算するためにそれらを変更、

9
00:00:29.340 --> 00:00:31.581
ほとんどすべて同時に。

10
00:00:31.581 --> 00:00:34.000
それでは、これを行う方法の詳細を見てみましょう。

11
00:00:34.000 --> 00:00:38.640
これらは、あなたがZ_1を計算する方法の前のビデオから我々が持っている4つの方程式でした

12
00:00:38.640 --> 00:00:39.960
A_1、A_2 Z_2と。

13
00:00:39.960 --> 00:00:41.325
そして、彼らはどのようにあなたを教え、

14
00:00:41.325 --> 00:00:43.515
入力特徴ベクトルxが与えられ、

15
00:00:43.515 --> 00:00:48.300
単一のトレーニング例えばy_hatに等しいあなたはA_2を生成するためにそれらを使用することができます。

16
00:00:48.300 --> 00:00:50.478
さて、あなたはm個の訓練例を持っている場合は、

17
00:00:50.478 --> 00:00:52.560
あなたは、たとえば、このプロセスを繰り返す必要があります

18
00:00:52.560 --> 00:00:57.720
最初のトレーニング例は、y_hat_1を計算するためにx_superscript_round_brackets_1

19
00:00:57.720 --> 00:01:00.395
それはあなたの最初のトレーニング例の予測です。

20
00:01:00.395 --> 00:01:03.795
その後X_2、予測y_hat_2を生成することを使用し、

21
00:01:03.795 --> 00:01:08.200
そしてその下に予測y_hat_mを生成するx_mします。

22
00:01:08.200 --> 00:01:11.430
そのため、同様に活性化関数表記でこれを書くために、

23
00:01:11.430 --> 00:01:15.200
私はa_2_square_brackets_round_bracket_1としてこれを書くつもりです。

24
00:01:15.200 --> 00:01:17.785
これは2_2とa_2_mです。

25
00:01:17.785 --> 00:01:22.670
したがって、この表記法、a_square_bracket_2_round_bracket_i、

26
00:01:22.670 --> 00:01:26.045
ラウンドbracket_iは訓練例を参照I

27
00:01:26.045 --> 00:01:29.570
正方形bracket_2は、二つの層を意味します。

28
00:01:29.570 --> 00:01:32.785
だから、正方形のブラケットと丸括弧インデックスがどのように動作するかです。

29
00:01:32.785 --> 00:01:34.620
そして、これがあれば、あなたが持っていることを示唆しています

30
00:01:34.620 --> 00:01:36.690
unvectorizedインデントとしたいです

31
00:01:36.690 --> 00:01:39.055
すべての訓練例に予測を計算、

32
00:01:39.055 --> 00:01:41.670
あなたは、iがmに1に等しいために行う必要があり、

33
00:01:41.670 --> 00:01:43.905
その後、基本的にはこれらの4次方程式を実装します。

34
00:01:43.905 --> 00:01:52.315
z_1_iは、a_1_iはz_1_iのシグモイドに等しい、w_1_i_1に等しいです。

35
00:01:52.315 --> 00:01:55.685
z_2_i等号

36
00:01:55.685 --> 00:02:03.390
W_2 a_1_i + B_2。

37
00:02:03.390 --> 00:02:07.250
これらの4つの方程式上に上付き文字丸括弧を追加します

38
00:02:07.250 --> 00:02:11.220
訓練例に依存するすべての変数iに、

39
00:02:11.220 --> 00:02:14.700
私はX、Zとに上付きラウンドブラケットを追加すること、

40
00:02:14.700 --> 00:02:15.930
あなたが計算したい場合は、

41
00:02:15.930 --> 00:02:18.480
あなたのメートルの訓練例のすべての出力。

42
00:02:18.480 --> 00:02:23.283
この式を取り除くように、私たちがやりたいことは、この全体の計算ベクトル化です。

43
00:02:23.283 --> 00:02:27.780
私は核心ザラザラ線形代数の多くを得ているようにところで、場合、それは、そうです

44
00:02:27.780 --> 00:02:29.935
それは、これを実装することができるということが判明します

45
00:02:29.935 --> 00:02:33.057
正しく深い学習誤差で重要であり、

46
00:02:33.057 --> 00:02:35.880
私たちは実際のために非常に慎重に表記を選択しました

47
00:02:35.880 --> 00:02:39.260
このコースとは、できるだけ簡単にこれらのvectorizationsを作りました。

48
00:02:39.260 --> 00:02:42.690
だから私は、この核心を通過すると、実際にあなたを助けることを願っています

49
00:02:42.690 --> 00:02:46.530
より迅速に作業これらのアルゴリズムの正しい実装を取得します。

50
00:02:46.530 --> 00:02:48.480
すべての権利は、そう私はちょうどコピーしてみましょう

51
00:02:48.480 --> 00:02:52.890
次のスライドへのコードのこの全体のブロックとし、我々はこれをベクトル化する方法について説明します。

52
00:02:52.890 --> 00:02:55.170
そこでここでは、と前のスライドのために持っているものです

53
00:02:55.170 --> 00:02:57.595
ループはすべてメートルの訓練例の上に行きます。

54
00:02:57.595 --> 00:03:01.770
だから私たちがする行列Xを定義することを思い出します

55
00:03:01.770 --> 00:03:06.585
そのように、これらの列に積み上げ、当社のトレーニング例に等しいです。

56
00:03:06.585 --> 00:03:08.235
だから、訓練例を取ります、

57
00:03:08.235 --> 00:03:09.503
列でそれらを積み重ね、

58
00:03:09.503 --> 00:03:14.540
これは、M次元のマトリックスによって多分n_x nまたはなります。

59
00:03:14.540 --> 00:03:17.340
私はパンチラインを離れて与え、必要に何を伝えるつもりです

60
00:03:17.340 --> 00:03:20.915
このforループのベクトル化実装を持つために実装します。

61
00:03:20.915 --> 00:03:26.260
あなたが何をする必要があるかが判明すると、計算Z_1はW_1 X + B_1に等しいです

62
00:03:26.260 --> 00:03:30.300
A_1は、Z_1のシグモイドに等しいです

63
00:03:30.300 --> 00:03:34.800
その後、Z_2は、W_2回A_1 + B_2に等しいです。

64
00:03:34.800 --> 00:03:39.295
その後、A_2はZ_2のシグモイドに等しいです。

65
00:03:39.295 --> 00:03:44.565
あなたが望むのであれば、アナロジーは、我々は小文字のベクトルxのから行ったということです

66
00:03:44.565 --> 00:03:50.430
異なる列に小文字のxのを積み重ねることによって、この資本ケースX行列へ。

67
00:03:50.430 --> 00:03:52.640
ですから、Zのために同じことを行う場合には、

68
00:03:52.640 --> 00:03:56.040
そう例えば、あなたがz_1_1を取る場合は、

69
00:03:56.040 --> 00:04:00.985
z_1_2など、これらすべての列は、右、z_1_mまでベクトル

70
00:04:00.985 --> 00:04:02.890
そのためには、この第1の量ですが、

71
00:04:02.890 --> 00:04:05.245
しかし、それらのすべてメートルとは、列にそれらを積み重ねます

72
00:04:05.245 --> 00:04:07.760
これはあなたの行列Z_1を与えます。

73
00:04:07.760 --> 00:04:11.850
同様に、あなたが言う、この量を見て、a_1_1取る場合、

74
00:04:11.850 --> 00:04:17.760
a_1_2などとa_1_mと列でそれらを積み重ね、

75
00:04:17.760 --> 00:04:20.040
これは、我々は小文字から行ったのと同様

76
00:04:20.040 --> 00:04:23.484
資本ケースZへの資本ケースXと小文字のzへのxの、

77
00:04:23.484 --> 00:04:26.370
これは、ベクトルである小文字のAから行きます

78
00:04:26.370 --> 00:04:29.700
あそここの資本A_1へ。

79
00:04:29.700 --> 00:04:32.490
そして同様に、Z_2とA_2のために、

80
00:04:32.490 --> 00:04:34.230
彼らはまた、撮影したです

81
00:04:34.230 --> 00:04:37.650
これらのベクターおよび水平にそれらを積み重ね、その後服用

82
00:04:37.650 --> 00:04:43.170
これらのベクターおよび資本Z_2と資本A_2を取得するために、水平にそれらを積み重ねます。

83
00:04:43.170 --> 00:04:45.230
役立つかもしれないこの表記の財産の一つ

84
00:04:45.230 --> 00:04:47.640
あなたはそれについて考えてこれらの行列ということです、

85
00:04:47.640 --> 00:04:49.575
、水平方向、ZとAを言います

86
00:04:49.575 --> 00:04:52.311
我々は、訓練例渡ってインデックスになるだろう

87
00:04:52.311 --> 00:04:57.200
水平方向のインデックスが異なるトレーニング例に対応してなぜそれがです。

88
00:04:57.200 --> 00:04:59.460
あなたは左から右にスイープとして、あなたはトレーニングセットをスキャンします。

89
00:04:59.460 --> 00:05:04.660
そして、垂直、この垂直インデックスは、ニューラルネットワーク内の異なるノードに対応しています。

90
00:05:04.660 --> 00:05:06.580
ですから、例えば、このノード、

91
00:05:06.580 --> 00:05:10.300
行列の最上部左端のコーナーでは、この値

92
00:05:10.300 --> 00:05:15.000
最初のトレーニングの例の最初の隠れユニットの活性化に相当し、

93
00:05:15.000 --> 00:05:16.710
一つの値がダウンに対応します

94
00:05:16.710 --> 00:05:20.558
最初のトレーニング例で第二の隠れユニットで活性化

95
00:05:20.558 --> 00:05:23.700
そして第三の隠れユニット最初のトレーニングの例でのように。

96
00:05:23.700 --> 00:05:25.045
ですから、ダウンスキャンとして、

97
00:05:25.045 --> 00:05:28.975
これは、あなたが隠されたユニットの数にインデックス付けされ、

98
00:05:28.975 --> 00:05:30.550
あなたが水平方向に移動した場合のに対し、

99
00:05:30.550 --> 00:05:33.275
あなたが最初のトレーニングの例では最初の隠れユニットからなるだろう

100
00:05:33.275 --> 00:05:36.295
今第2の例の最初の隠されたユニット、

101
00:05:36.295 --> 00:05:41.890
ように第三の訓練例及びここでこのノードが活性化に対応するまで

102
00:05:41.890 --> 00:05:47.940
m番目のトレーニング例の最終的な訓練例の最初の隠れユニット。

103
00:05:47.940 --> 00:05:54.025
だから、行列Aは、私たちのさまざまな訓練例の上に横になり、

104
00:05:54.025 --> 00:05:56.320
中および垂直方向に、異なる指標

105
00:05:56.320 --> 00:05:59.770
行列Aは、異なる隠れユニットに相当します。

106
00:05:59.770 --> 00:06:04.780
そして、同様の直感は、行列Zのためにも当てはまるだけでなく、Xのためでした

107
00:06:04.780 --> 00:06:08.345
ここで、水平方向に異なるトレーニング例に対応し、

108
00:06:08.345 --> 00:06:12.580
垂直異なる入力機能に対応し、

109
00:06:12.580 --> 00:06:16.695
これは本当にニューラルネットワークの入力層内の異なるノードです。

110
00:06:16.695 --> 00:06:19.855
だから、あなただけのこれらの4つの方程式を実装する場合、

111
00:06:19.855 --> 00:06:21.940
あなたは正しい答えを得るでしょう。

112
00:06:21.940 --> 00:06:23.545
しかし、片側だけに、

113
00:06:23.545 --> 00:06:26.260
私はのために少しより多くの正当性を与えてみましょう

114
00:06:26.260 --> 00:06:29.335
なぜこれが正しいベクトルの実装です。

115
00:06:29.335 --> 00:06:32.495
そして、推論は、あなたが持っているものと非常によく似であることが判明しました

116
00:06:32.495 --> 00:06:35.985
ロジスティック回帰でベクトル化のために見られます。

117
00:06:35.985 --> 00:06:41.405
それでは、いくつかの例については、前方伝播計算の一部を通過してみましょう。

118
00:06:41.405 --> 00:06:43.340
のは、最初のトレーニング例のためにそれを言ってみましょう、

119
00:06:43.340 --> 00:06:48.456
あなたはこの、X_1 + B_1を計算してしまいます。

120
00:06:48.456 --> 00:06:50.890
そして、第2のトレーニング例えば、

121
00:06:50.890 --> 00:06:55.090
あなたは、この+ B_1 2を計算してしまいます。

122
00:06:55.090 --> 00:06:56.970
そして、第三例えば、

123
00:06:56.970 --> 00:06:59.475
あなたはこれを計算に終わります。

124
00:06:59.475 --> 00:07:03.220
だからスライド上の説明を簡単にするために、私はBを無視するつもりです。

125
00:07:03.220 --> 00:07:06.875
それでは、ちょうどこの正当性を少し簡単にするために、言わせて、

126
00:07:06.875 --> 00:07:08.560
そのBは、ゼロに等しいです。

127
00:07:08.560 --> 00:07:11.110
しかし、我々はレイアウトするつもりだ引数がで動作します

128
00:07:11.110 --> 00:07:14.090
bがゼロでも、変更のほんの少し、

129
00:07:14.090 --> 00:07:17.010
しかし、これはただのスライドに少し説明を簡単にするためです。

130
00:07:17.010 --> 00:07:20.060
さて、W_1は、いくつかの行列になるだろう。

131
00:07:20.060 --> 00:07:22.960
だから私は、この行列の行をいくつか持っています。

132
00:07:22.960 --> 00:07:25.570
ですから、この計算X_1を見れば、

133
00:07:25.570 --> 00:07:31.385
何を持っていることは、あなたにいくつかの列ベクトルを与える変数x_1 W_1倍ということです

134
00:07:31.385 --> 00:07:33.455
これは私がこのように描画するつもりです。

135
00:07:33.455 --> 00:07:37.280
同様に、あなたはこのベクトルX_2を見れば、

136
00:07:37.280 --> 00:07:42.765
あなたはW_1回X_2は、他のいくつかの列ベクトルを与える、ということがあります。

137
00:07:42.765 --> 00:07:45.340
それはあなたにこのz_1_2を与えます。

138
00:07:45.340 --> 00:07:47.395
そして最後に、あなたがX_3を見れば、

139
00:07:47.395 --> 00:07:53.660
あなたはあなたにいくつかの3番目の列ベクトルを与えるX_3 W_1回を持っています。

140
00:07:53.660 --> 00:07:55.630
つまり、このz_1_3です。

141
00:07:55.630 --> 00:07:59.760
だから今、あなたはトレーニングセットを考慮した場合

142
00:07:59.760 --> 00:08:04.990
私たちは一緒に私たちの訓練例の全てを積層して形成され、資本X、

143
00:08:04.990 --> 00:08:09.025
その行列資本Xは、ベクトルX_1を取ることによって形成されています。

144
00:08:09.025 --> 00:08:13.670
そして、X_2で垂直に積み重ね、その後もX_3。

145
00:08:13.670 --> 00:08:16.523
我々は3つしか訓練例を持っている場合、これはあります。あなたはより多くを持っている場合、

146
00:08:16.523 --> 00:08:19.875
私たちはそのように水平に積み重ねておこう。

147
00:08:19.875 --> 00:08:24.160
しかし、あなたは今、この行列Xを取り、Wを掛けた場合、

148
00:08:24.160 --> 00:08:25.257
あなたがで終わります、

149
00:08:25.257 --> 00:08:27.430
あなたは行列の乗算がどのように機能するかを考える場合、

150
00:08:27.430 --> 00:08:29.410
あなたは、最初の列があることで終わります

151
00:08:29.410 --> 00:08:33.360
私は紫色でそこまで描かれていたこれらの同じ値。

152
00:08:33.360 --> 00:08:36.721
第2列は、これらの同じ4つの値となり、

153
00:08:36.721 --> 00:08:41.045
3列目は、彼らがあることが判明するものを、それらのオレンジ色の値となります。

154
00:08:41.045 --> 00:08:47.525
もちろん、これは列ベクトルとしてz_1_1にちょうど等しく発現され

155
00:08:47.525 --> 00:08:50.140
z_1_2続いとして発現

156
00:08:50.140 --> 00:08:54.540
z_1_3続く列ベクトルはまた、列ベクトルとして表現しました

157
00:08:54.540 --> 00:08:56.347
あなたは3つの訓練例を持っている場合、これはあります。

158
00:08:56.347 --> 00:08:57.455
あなたはより多くの例を持っている場合は、

159
00:08:57.455 --> 00:08:59.360
その後、多くの列があります。

160
00:08:59.360 --> 00:09:03.765
そして、これは私たちの行列資本Z_1です。

161
00:09:03.765 --> 00:09:09.160
だから私は、これは我々が以前に持っていたとき、なぜを正当化する理由を与える願っていますw_i

162
00:09:09.160 --> 00:09:16.885
我々は一度に単一の訓練例を見ているときX_Iがz_1_iに等しい回、

163
00:09:16.885 --> 00:09:20.590
あなたは別の例を取り、別の列でそれらを積み上げたときに、

164
00:09:20.590 --> 00:09:26.275
その後、対応する結果は、あなたは、zのも列に積み上げで終わるということです。

165
00:09:26.275 --> 00:09:28.720
そして、私は表示されませんが、あなたは場合は、自分自身を納得させることができます

166
00:09:28.720 --> 00:09:31.345
あなたは、Pythonの放送であることをしたいです、

167
00:09:31.345 --> 00:09:33.820
あなたはBのこれらの値に戻って追加した場合、

168
00:09:33.820 --> 00:09:36.730
値がまだ正しいことと

169
00:09:36.730 --> 00:09:39.828
どのような実際に起こって終了すると、Pythonの放送で終わるあり、

170
00:09:39.828 --> 00:09:44.765
あなたは、この行列の列のそれぞれに個別にb_iを追加してしまいます。

171
00:09:44.765 --> 00:09:49.420
したがって、このスライドに、私は唯一のZ_1がW_1 Xに等しいことを正当化してきました

172
00:09:49.420 --> 00:09:53.740
#ERROR!

173
00:09:53.740 --> 00:09:56.616
私たちは前のスライドの上に持っている4つのステップの最初のステップの、

174
00:09:56.616 --> 00:09:58.810
それは同様の解析があなたを可能にすることが判明します

175
00:09:58.810 --> 00:10:01.080
他のステップでも動作することを示すために

176
00:10:01.080 --> 00:10:05.800
あなたが列に入力を積み重ねる場合に非常に類似したロジックを使用して、

177
00:10:05.800 --> 00:10:07.045
その後、式の後、

178
00:10:07.045 --> 00:10:09.970
あなたはまた、列に積み上げ対応する出力を得ます。

179
00:10:09.970 --> 00:10:13.505
最後に、ちょうど私たちは、このビデオで話題のすべてをおさらいしましょう。

180
00:10:13.505 --> 00:10:14.950
これはあなたのニューラルネットワークである場合は、

181
00:10:14.950 --> 00:10:17.770
私たちは、あなたがした場合、これはあなたが何をする必要があるかであることを言いました

182
00:10:17.770 --> 00:10:21.195
一度に前方伝播1のトレーニング例を実装、

183
00:10:21.195 --> 00:10:24.130
私から行くことからmまで1に等しいです。そして、我々は言いました、

184
00:10:24.130 --> 00:10:27.340
さんがそうのような列の訓練例を積み重ねてみましょう、

185
00:10:27.340 --> 00:10:29.785
これらの値をZ_1のそれぞれについて、

186
00:10:29.785 --> 00:10:34.180
A_1、Z_2、A_2、次のようにのは、対応する列を積み重ねてみましょう。

187
00:10:34.180 --> 00:10:36.820
したがって、この例では、キャップA_1ため、

188
00:10:36.820 --> 00:10:42.110
これはZ_1、A_1、A_2 Z_2とについても同様です。

189
00:10:42.110 --> 00:10:46.360
そして、この行ができること、私たちが以前のフライトであったました

190
00:10:46.360 --> 00:10:50.665
あなたは、同時にすべてのm個の例を越え、これをベクトル化します。

191
00:10:50.665 --> 00:10:52.240
そして、それは同様の推論であることが判明し、

192
00:10:52.240 --> 00:10:55.165
あなたは、他のすべての行があることを示すことができます

193
00:10:55.165 --> 00:10:58.840
これらのコード行の4つのすべての正しいvectorizations。

194
00:10:58.840 --> 00:11:00.355
そして、ちょうどリマインダーとして、

195
00:11:00.355 --> 00:11:03.885
Xはまた、A_0に等しいので、なぜなら

196
00:11:03.885 --> 00:11:07.930
、入力特徴ベクトルxがA_0に等しかったことを、覚えておいてください

197
00:11:07.930 --> 00:11:11.235
そうX_Iは、a_0_i等しいです

198
00:11:11.235 --> 00:11:16.360
この最初の方程式これらの方程式の特定の対称性が実際にあります

199
00:11:16.360 --> 00:11:22.150
また、書き込むことができZ_1はW_1 A_0プラスB_1に等しいです。

200
00:11:22.150 --> 00:11:24.490
だから、あなたが見ることのこのペア

201
00:11:24.490 --> 00:11:27.677
方程式と方程式のこのペアは、実際には非常に似て、

202
00:11:27.677 --> 00:11:30.300
ちょうど指標の全てを一つ進めます。

203
00:11:30.300 --> 00:11:34.320
ニューラルネットワークの異なる層であることを示しているのだから、この種

204
00:11:34.320 --> 00:11:38.733
ほぼ同じことをやってか、単に何度も同じ計算を行います。

205
00:11:38.733 --> 00:11:40.930
そして、ここでは2層ニューラルネットワークを持っています。

206
00:11:40.930 --> 00:11:45.880
我々は来週の動画でより深いニューラルネットワークに行くとき、

207
00:11:45.880 --> 00:11:49.270
あなたも、より深いニューラルネットワークは基本的に服用していることがわかり

208
00:11:49.270 --> 00:11:53.149
この2つのステップとあなたがここで見るよりも、単にそれらを行うと、さらに倍。

209
00:11:53.149 --> 00:11:58.100
だから、それはあなたが複数の訓練例にまたがってニューラルネットワークをベクトル化することができます方法です。

210
00:11:58.100 --> 00:12:02.435
次に、我々はこれまで、当社のニューラルネットワーク全体でシグモイド関数を使用してきました。

211
00:12:02.435 --> 00:12:04.760
それは実際には最良の選択ではないのですが判明。

212
00:12:04.760 --> 00:12:08.470
次のビデオでは、あなたがどのように使用できるかに、さらに少し掘り下げてみましょう

213
00:12:08.470 --> 00:12:13.600
異なる活性化関数はシグモイド関数は、単に可能一つであります
