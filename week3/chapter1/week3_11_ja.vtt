WEBVTT

1
00:00:00.060 --> 00:00:02.540
あなたは、ニューラルネットワークがある訓練するとき

2
00:00:01.319 --> 00:00:05.549
重みを初期化することが重要

3
00:00:02.540 --> 00:00:07.500
ランダムロジスティック回帰のためにそれがでした

4
00:00:05.549 --> 00:00:09.780
ゼロに重みを初期化しても大丈夫

5
00:00:07.500 --> 00:00:12.059
しかし、初期の新しいネットワークのための

6
00:00:09.780 --> 00:00:14.460
廃棄物のすべてをゼロにパラメータの

7
00:00:12.059 --> 00:00:19.890
それは動作しません勾配降下を適用します

8
00:00:14.460 --> 00:00:23.460
あなたがここに二つの入力を持っている理由を見てみましょう

9
00:00:19.890 --> 00:00:26.369
機能N0は、2及び2に等しいので

10
00:00:23.460 --> 00:00:30.660
隠れユニットn1が2に等しいように

11
00:00:26.369 --> 00:00:35.700
したがってマトリックスは、隠された関連付けられました

12
00:00:30.660 --> 00:00:38.510
層またはW1はそれがあることを行っている2×2してみましょう

13
00:00:35.700 --> 00:00:42.809
あなたがすべてゼロに初期化することを言います

14
00:00:38.510 --> 00:00:46.469
0 0 0 0 2行列で2とのは、P1を言わせて

15
00:00:42.809 --> 00:00:49.710
0 0にも等しいが判明しています

16
00:00:46.469 --> 00:00:52.440
0にバイアス用語Bが初期化されます

17
00:00:49.710 --> 00:00:55.530
実際には大丈夫しかし、初期化にW

18
00:00:52.440 --> 00:00:57.420
すべてのゼロが問題となるような問題です

19
00:00:55.530 --> 00:01:00.870
この正式な初期化があるということで

20
00:00:57.420 --> 00:01:09.270
どんなたとえば、あなたが持っているそれを与えます

21
00:01:00.870 --> 00:01:11.250
1 1、1 2は、すべて等しくなること

22
00:01:09.270 --> 00:01:13.439
右ので、この活性化し、この

23
00:01:11.250 --> 00:01:15.000
活性化は、両方のために同じになります

24
00:01:13.439 --> 00:01:18.240
これらの隠れユニットが正確に計算されています

25
00:01:15.000 --> 00:01:20.610
同じ機能と、その後とき

26
00:01:18.240 --> 00:01:28.799
結局のところ伝播をバック計算

27
00:01:20.610 --> 00:01:30.810
そのD Z1 1とDのZ1 2にもなります

28
00:01:28.799 --> 00:01:32.280
対称性によって、同じ種類の右の両方の

29
00:01:30.810 --> 00:01:36.060
これらの隠れユニットが初期化されます

30
00:01:32.280 --> 00:01:38.040
私が言っている何のために技術的に同じ方法

31
00:01:36.060 --> 00:01:42.750
私は、発信重みと仮定しています

32
00:01:38.040 --> 00:01:48.119
W 2が等しくなるように、また同じです

33
00:01:42.750 --> 00:01:51.479
0 0にはできますが、神経を初期化した場合

34
00:01:48.119 --> 00:01:53.369
ネットワークこうして、この隠されたユニット

35
00:01:51.479 --> 00:01:54.780
この隠されたユニットは完全にあります

36
00:01:53.369 --> 00:01:56.579
同じように、彼らは完全にしています

37
00:01:54.780 --> 00:01:57.930
時々、あなたは彼らが完全だと言います

38
00:01:56.579 --> 00:02:00.450
ちょうどことを意味し、対称

39
00:01:57.930 --> 00:02:04.619
まったく同じ関数を計算し、

40
00:02:00.450 --> 00:02:07.110
誘導による証拠の種類によってそれは変わります

41
00:02:04.619 --> 00:02:09.060
のすべての単一の反復後、そのアウト

42
00:02:07.110 --> 00:02:10.739
あなたは、2つの隠しユニットがされている訓練します

43
00:02:09.060 --> 00:02:11.680
まだ正確に同じ混乱

44
00:02:10.739 --> 00:02:15.220
関数

45
00:02:11.680 --> 00:02:18.159
DWがなるショーの一部以来、

46
00:02:15.220 --> 00:02:21.310
これはあらゆるところのように見える行列

47
00:02:18.159 --> 00:02:23.560
行は同じ値をとるので、

48
00:02:21.310 --> 00:02:26.440
あなたが行うので、重み更新を行います

49
00:02:23.560 --> 00:02:30.430
あなたは重み更新W1が取得を実行するとき

50
00:02:26.440 --> 00:02:33.640
W1マイナスアルファ回DWあなたのように更新

51
00:02:30.430 --> 00:02:36.970
すべての反復後にそのW1を見つけるだろう

52
00:02:33.640 --> 00:02:39.099
あなたは、に等しい最初の行を知っています

53
00:02:36.970 --> 00:02:41.170
それが構築することが可能ですので、第二列

54
00:02:39.099 --> 00:02:43.569
誘導による証拠ます場合は、その

55
00:02:41.170 --> 00:02:47.500
すべての方法にすべての値を初期化します

56
00:02:43.569 --> 00:02:49.450
Wその後、理由は隠れユニットの両方を0にします

57
00:02:47.500 --> 00:02:51.549
同じ関数を計算始めます

58
00:02:49.450 --> 00:02:55.030
そして両方の隠れユニットは同じを持っています

59
00:02:51.549 --> 00:02:57.609
出力ユニットその後、後に及ぼす影響

60
00:02:55.030 --> 00:02:59.379
同じ文があることを一回の反復

61
00:02:57.609 --> 00:03:01.180
まだ満たされている単位で記入します

62
00:02:59.379 --> 00:03:03.010
まだ対称したがってによって、

63
00:03:01.180 --> 00:03:05.109
誘導後の2回の反復3

64
00:03:03.010 --> 00:03:07.150
どんなに上の反復のでどのくらい

65
00:03:05.109 --> 00:03:09.519
あなたのネットワークの両方隠しで訓練します

66
00:03:07.150 --> 00:03:11.769
単位は、まだ正確に計算されています

67
00:03:09.519 --> 00:03:14.109
この場合、同一の機能及びそう

68
00:03:11.769 --> 00:03:15.669
より多くを有することに何のポイントは本当にありません

69
00:03:14.109 --> 00:03:17.889
1つの隠れユニットよりも、彼らはすべてのだから

70
00:03:15.669 --> 00:03:20.470
同じことを計算し、講座の

71
00:03:17.889 --> 00:03:22.660
大規模ニューラルネットワークのためのより少ないあなたが持っています

72
00:03:20.470 --> 00:03:24.819
3つの特徴と、おそらく非常に大きいです

73
00:03:22.660 --> 00:03:28.419
同様の隠れユニットの数

74
00:03:24.819 --> 00:03:31.450
引数には、どのような新しいことを示すために働きます

75
00:03:28.419 --> 00:03:33.849
私は描かれないので、このようなネットワーク

76
00:03:31.450 --> 00:03:35.919
すべてのエッジは、あなたが道を初期化した場合

77
00:03:33.849 --> 00:03:38.769
その後、あなたの頭のエンジニアのすべてをゼロに

78
00:03:35.919 --> 00:03:40.030
対称であり、どんなに長います

79
00:03:38.769 --> 00:03:42.519
勾配降下を実行して、彼らはすべてのよ

80
00:03:40.030 --> 00:03:45.519
まったく同じ計算をし続けます

81
00:03:42.519 --> 00:03:47.680
そのように機能が便利ではありませんので、

82
00:03:45.519 --> 00:03:50.260
次の2つの異なる隠れユニットにしたいです

83
00:03:47.680 --> 00:03:52.480
異なる機能に解を計算

84
00:03:50.260 --> 00:03:56.379
これにパラメータを初期化することです

85
00:03:52.480 --> 00:04:02.590
ランダムので、ここであなたができる仕事です

86
00:03:56.379 --> 00:04:04.870
集合W 1は、NPが持っ等しいランダムドットRとn

87
00:04:02.590 --> 00:04:09.010
この私たちのガウスランダムに生成

88
00:04:04.870 --> 00:04:10.720
変数2にして、通常ます

89
00:04:09.010 --> 00:04:13.540
非常に少ない数で、これを掛けます

90
00:04:10.720 --> 00:04:17.190
そのようなので、あなたがそれを初期化0.01

91
00:04:13.540 --> 00:04:20.650
非常に小さなランダムな値と、その後もええと

92
00:04:17.190 --> 00:04:22.630
それはBがこれを持っていないことが判明します

93
00:04:20.650 --> 00:04:24.650
呼ばれるものを対称問題

94
00:04:22.630 --> 00:04:29.570
対称性の破れ問題

95
00:04:24.650 --> 00:04:31.699
ので、ちょうどゼロにBを初期化しても大丈夫です

96
00:04:29.570 --> 00:04:33.620
限りWが初期化されているため

97
00:04:31.699 --> 00:04:35.449
ランダムにあなたが始めます

98
00:04:33.620 --> 00:04:37.130
コンピューティング異なる隠れユニット

99
00:04:35.449 --> 00:04:39.560
ですから、もはや別物と

100
00:04:37.130 --> 00:04:42.650
このUM対称性の破れ問題を抱えています

101
00:04:39.560 --> 00:04:47.600
その後、同様にW2のためのことができます。

102
00:04:42.650 --> 00:04:51.289
ランダムにそれを初期化し、B2ことができます

103
00:04:47.600 --> 00:04:52.639
あなたがかもしれないので、ゼロにそれを初期化します

104
00:04:51.289 --> 00:04:55.130
これをしなかった場所がわから不思議

105
00:04:52.639 --> 00:04:59.389
定数から来て、なぜそれが0.01であります

106
00:04:55.130 --> 00:05:01.820
なぜ数100または1000ターンを入れません

107
00:04:59.389 --> 00:05:03.620
私たちは通常、初期化することを好むことを

108
00:05:01.820 --> 00:05:07.660
非常に非常に小さなランダムにする方法

109
00:05:03.620 --> 00:05:10.400
値umのためにあなたが使用している場合

110
00:05:07.660 --> 00:05:12.710
悪魔やシグモイド活性化関数

111
00:05:10.400 --> 00:05:15.289
またはあなたはシグモイドを持っている場合でも、ただで

112
00:05:12.710 --> 00:05:18.289
出力層波があまりにある場合

113
00:05:15.289 --> 00:05:22.580
あなたが計算するときに大

114
00:05:18.289 --> 00:05:30.710
活性値は、Z1があることを覚えておいてください

115
00:05:22.580 --> 00:05:35.599
次いで、1は上でW1のXプラスBと等しいです

116
00:05:30.710 --> 00:05:39.050
活性化関数はそうZ1に適用しました

117
00:05:35.599 --> 00:05:41.360
Wは非常に大きい場合にはZは非常に大きなものとなりますか

118
00:05:39.050 --> 00:05:45.260
Zのこれらのいくつかの値のいずれかになります

119
00:05:41.360 --> 00:05:46.970
非常に大きいまたは非常に小さいので、その中

120
00:05:45.260 --> 00:05:50.840
場合あなたがで終わる可能性が高くなります

121
00:05:46.970 --> 00:05:53.659
10H機能のこれらの平坦部または

122
00:05:50.840 --> 00:05:55.610
シグモイド関数場合の傾き

123
00:05:53.659 --> 00:05:57.740
勾配は非常に小さいです

124
00:05:55.610 --> 00:05:59.479
勾配降下があろうことを意味

125
00:05:57.740 --> 00:06:00.080
非常に遅いので、非常だろう学習

126
00:05:59.479 --> 00:06:04.039
スロー

127
00:06:00.080 --> 00:06:05.900
これだけ復習Wが大きすぎる場合は、しています

128
00:06:04.039 --> 00:06:07.789
でも、非常に終わる可能性が高いです

129
00:06:05.900 --> 00:06:10.880
非常に大きな値とトレーニングの開始

130
00:06:07.789 --> 00:06:13.060
シグモイドにあなたの10の原因となるZの

131
00:06:10.880 --> 00:06:16.430
活性化関数は、上で飽和します

132
00:06:13.060 --> 00:06:18.199
そうしない場合は学習鈍化

133
00:06:16.430 --> 00:06:19.789
任意の1または10の各活性化を言って持っています

134
00:06:18.199 --> 00:06:22.400
あなたのニューラルネットワーク全体の機能

135
00:06:19.789 --> 00:06:24.169
これは、問題の少ないですが、している場合

136
00:06:22.400 --> 00:06:26.720
バイナリ分類を行うと、あなたの

137
00:06:24.169 --> 00:06:28.400
出力ユニットはシグモイド関数であります

138
00:06:26.720 --> 00:06:31.250
あなたは、あなただけの最初のを望んでいない知っています

139
00:06:28.400 --> 00:06:34.219
それが理由ですので、パラメータが大きすぎることにします

140
00:06:31.250 --> 00:06:36.560
0.01で乗算するものになるだろう

141
00:06:34.219 --> 00:06:38.810
試して合理的またはその他の小さな

142
00:06:36.560 --> 00:06:44.000
W2の番号と同じ

143
00:06:38.810 --> 00:06:47.419
これは私がこれを推測ランダムランダムすることができ

144
00:06:44.000 --> 00:06:54.530
この例の回では2で1になります

145
00:06:47.419 --> 00:06:56.930
0.01シグマのがそう最終的にそれが変わります

146
00:06:54.530 --> 00:07:01.460
そのうち時には彼らが改善することができます

147
00:06:56.930 --> 00:07:04.490
あなたが訓練している0.01より定数

148
00:07:01.460 --> 00:07:06.919
隠された一つだけとニューラルネットワーク

149
00:07:04.490 --> 00:07:09.100
これは比較的浅い層、あなたの

150
00:07:06.919 --> 00:07:12.200
あまりにも多くの隠れ層のないネットワーク

151
00:07:09.100 --> 00:07:14.240
0.01に設定すると、おそらく正常に動作します

152
00:07:12.200 --> 00:07:16.729
しかし、あなたは非常に非常に訓練しているとき

153
00:07:14.240 --> 00:07:19.010
深いニューラルネットワークは、あなたが欲しいかもしれません

154
00:07:16.729 --> 00:07:21.950
0.001に異なる定数を選択します

155
00:07:19.010 --> 00:07:24.350
そして来週の材料に話すことはありません

156
00:07:21.950 --> 00:07:26.240
いつ、どのようにあなたがかもしれないについて少し

157
00:07:24.350 --> 00:07:29.720
異なる定数を選びたいです

158
00:07:26.240 --> 00:07:32.000
0.01しかし、それは、通常は終了しますいずれかの方法

159
00:07:29.720 --> 00:07:34.850
アップので、比較的少数であること

160
00:07:32.000 --> 00:07:38.000
それはそう今週の動画を今すぐそれです

161
00:07:34.850 --> 00:07:40.040
のニューラルネットワークをセットアップする方法を知っています

162
00:07:38.000 --> 00:07:42.139
隠れ層は、パラメータを初期化します

163
00:07:40.040 --> 00:07:44.150
前方に使用して予測を行うよう支えます

164
00:07:42.139 --> 00:07:46.700
うまく計算誘導体のような、それが中です

165
00:07:44.150 --> 00:07:49.220
勾配降下するように小道具を使用してバック

166
00:07:46.700 --> 00:07:51.919
あなたはとしてクイズを行うことができるはず

167
00:07:49.220 --> 00:07:53.600
だけでなく、病気のベスト演習をカバー

168
00:07:51.919 --> 00:07:55.610
それと運の私はあなたが楽しみを持っている願っています

169
00:07:53.600 --> 00:07:59.410
私の運動としてを楽しみにして

170
00:07:55.610 --> 00:07:59.410
二週間涼しい素材であなたを見て