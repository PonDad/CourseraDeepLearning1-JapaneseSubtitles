WEBVTT

1
00:00:00.030 --> 00:00:03.629
この第四週へようこそ

2
00:00:01.589 --> 00:00:05.609
今、あなたが前方に見てきたことでコース

3
00:00:03.629 --> 00:00:07.560
での伝播と逆伝播

4
00:00:05.609 --> 00:00:09.599
ニューラルネットワークの文脈

5
00:00:07.560 --> 00:00:11.519
単一の隠れ層だけでなく、ロジスティック

6
00:00:09.599 --> 00:00:13.620
回帰あなたはについて学びました

7
00:00:11.519 --> 00:00:14.820
ベクトルとするとき、それは重要です

8
00:00:13.620 --> 00:00:16.890
体重を初期化

9
00:00:14.820 --> 00:00:18.210
ランダムにあなたが過去にやった場合

10
00:00:16.890 --> 00:00:20.039
我々はまた、きた、同社の宿題

11
00:00:18.210 --> 00:00:22.410
実装されており、これらのアイデアのいくつかを見て

12
00:00:20.039 --> 00:00:24.150
そう、あなたがきた今では、自分自身のために働きます

13
00:00:22.410 --> 00:00:26.849
実際に必要なアイデアのほとんどを見て

14
00:00:24.150 --> 00:00:28.109
深いニューラルネットワークを実装するためにどのような

15
00:00:26.849 --> 00:00:30.269
私たちは、今週中にやろうとしている取るです

16
00:00:28.109 --> 00:00:31.619
これらのアイデアので、それらを一緒に入れて

17
00:00:30.269 --> 00:00:34.590
あなたが実装できるようになりますことをご

18
00:00:31.619 --> 00:00:37.440
この我々ので自分の深いニューラルネットワーク

19
00:00:34.590 --> 00:00:39.450
おそらく運動は長く、ちょうど持っています

20
00:00:37.440 --> 00:00:41.640
ビデオを維持するつもりもう少し仕事

21
00:00:39.450 --> 00:00:43.140
あなたが得るように今週短いです

22
00:00:41.640 --> 00:00:45.079
動画もう少し迅速かつ

23
00:00:43.140 --> 00:00:47.219
その後、私が行うにはより多くの時間を持っています

24
00:00:45.079 --> 00:00:49.890
最後に重要なプログラムの演習

25
00:00:47.219 --> 00:00:51.360
私が建てたあなたを残していることを願っています

26
00:00:49.890 --> 00:00:55.230
あなたが感じる深いニューラルネットワーク

27
00:00:51.360 --> 00:00:57.180
ハンノキはとても深いニューラルネットワークは何ですか

28
00:00:55.230 --> 00:01:00.780
あなたは識字のために、この絵を見てきました

29
00:00:57.180 --> 00:01:03.690
回帰ます。また、新しい見てきました

30
00:01:00.780 --> 00:01:05.909
その単一の隠れ層を持つネットワーク

31
00:01:03.690 --> 00:01:08.220
ここでは、ニューラルネットワークの一例です

32
00:01:05.909 --> 00:01:11.250
2つの隠れ層とし、あなたの中に

33
00:01:08.220 --> 00:01:13.640
5つの隠れ層を持つネットワーク我々は言います

34
00:01:11.250 --> 00:01:18.330
そのロジスティック回帰は非常にあります

35
00:01:13.640 --> 00:01:21.780
このモデルはここにあるのに対し、浅いモデル

36
00:01:18.330 --> 00:01:24.150
より深いモデルと浅い対

37
00:01:21.780 --> 00:01:26.549
深さはとても神経の程度の問題です

38
00:01:24.150 --> 00:01:29.579
単一の隠れ層のネットワークこの

39
00:01:26.549 --> 00:01:31.770
2層ニューラルネットワークになります

40
00:01:29.579 --> 00:01:33.630
私たちはあなたにレイヤーを数える際に覚えています

41
00:01:31.770 --> 00:01:36.570
ネットワーク我々は、入力層はカウントされません。

42
00:01:33.630 --> 00:01:39.180
私たちは、同様の隠れ層を数えます

43
00:01:36.570 --> 00:01:42.780
出力層としてので、これは次のようになります

44
00:01:39.180 --> 00:01:45.000
ニューラルネットワークがまだかなりある2層

45
00:01:42.780 --> 00:01:47.070
浅いが、ロジスティックほど浅くありません

46
00:01:45.000 --> 00:01:49.380
回帰技術ロジスティック

47
00:01:47.070 --> 00:01:51.659
あなたが一つの層を知って回帰があります

48
00:01:49.380 --> 00:01:52.110
ニューラルネットワークが、いくつかの最後のオーバー

49
00:01:51.659 --> 00:01:54.720
年

50
00:01:52.110 --> 00:01:56.909
DAIや機械学習コミュニティ

51
00:01:54.720 --> 00:01:59.159
機能があることを認識しています

52
00:01:56.909 --> 00:02:02.460
その非常に深いニューラルネットワーク学習します

53
00:01:59.159 --> 00:02:05.310
浅いモデルは、多くの場合、できないこと

54
00:02:02.460 --> 00:02:06.960
ただし、任意の問題のために、それまで

55
00:02:05.310 --> 00:02:09.679
事前に予測することは難しいかもしれません

56
00:02:06.960 --> 00:02:12.019
正確にどのように深いニューラルネットワークます

57
00:02:09.679 --> 00:02:14.209
1ので、しようとするのが妥当だろう

58
00:02:12.019 --> 00:02:16.430
ロジスティック回帰は、1を試し、その後2

59
00:02:14.209 --> 00:02:18.409
隠れ層との数を表示

60
00:02:16.430 --> 00:02:20.989
別のハイパーパラメータとして隠れ層

61
00:02:18.409 --> 00:02:24.290
あなたは、種々の値を試すことができること

62
00:02:20.989 --> 00:02:26.450
ホールドアウトクロスに評価するの

63
00:02:24.290 --> 00:02:29.090
検証データや、すべての開発

64
00:02:26.450 --> 00:02:31.069
セットには、後にもそれについてより多くを語ります

65
00:02:29.090 --> 00:02:33.560
それでは表記我々見ていきましょう

66
00:02:31.069 --> 00:02:38.090
深いニューラルネットワークを記述するために使用

67
00:02:33.560 --> 00:02:42.319
ここに1 2 3 4層です

68
00:02:38.090 --> 00:02:45.769
3つの隠れ層を持つニューラルネットワーク

69
00:02:42.319 --> 00:02:49.069
これらのユニット数は、隠されました

70
00:02:45.769 --> 00:02:51.290
層では5 5 3オールマンができます

71
00:02:49.069 --> 00:02:52.489
ので、1つの出力ユニットがあります

72
00:02:51.290 --> 00:02:55.609
表記は、我々はそれが起こっているのに使用するつもりです

73
00:02:52.489 --> 00:02:57.680
の数を示すために、資本Lを使用します

74
00:02:55.609 --> 00:03:00.290
この場合、Lで非常にネットワーク内の層

75
00:02:57.680 --> 00:03:04.639
4に等しく、そのためです

76
00:03:00.290 --> 00:03:09.980
層の数と、我々が使用するつもり

77
00:03:04.639 --> 00:03:14.859
の数を示すために、N付き文字L

78
00:03:09.980 --> 00:03:19.099
ノートやそこでのユニット数

79
00:03:14.859 --> 00:03:24.229
小文字のLもしそうなら、私たちのインデックスこの

80
00:03:19.099 --> 00:03:27.199
層0として入力これはこれが層1であります

81
00:03:24.229 --> 00:03:32.319
層2これは、レイヤ3であり、これは

82
00:03:27.199 --> 00:03:35.239
層4その後、我々はN例えばそれを持っています

83
00:03:32.319 --> 00:03:37.579
この最初のではないだろう1

84
00:03:35.239 --> 00:03:40.459
なぜなら、私たち層は5に等しくなります

85
00:03:37.579 --> 00:03:43.220
この1のためにそこに5つの隠れユニットを持っています

86
00:03:40.459 --> 00:03:45.650
我々は、n 2にあるユニットの数を持っています

87
00:03:43.220 --> 00:03:54.459
そこに第2のセットはまた、に等しいです。

88
00:03:45.650 --> 00:03:57.919
5 O n 3は3およびn nは4に等しいです。

89
00:03:54.459 --> 00:03:59.750
資本Lは、ユニットのこの数はこれです

90
00:03:57.919 --> 00:04:02.540
出力ユニットの数はあなたのものです

91
00:03:59.750 --> 00:04:05.569
ここでは、当社の資本Lは4に等しいので、

92
00:04:02.540 --> 00:04:09.500
そして我々はまた、ここでその必要があるとしています

93
00:04:05.569 --> 00:04:13.370
従って、入力層と0がちょうど等しくなります

94
00:04:09.500 --> 00:04:15.470
NXにいいようだ3に等しく、

95
00:04:13.370 --> 00:04:17.329
我々が記述するために使用します記法

96
00:04:15.470 --> 00:04:20.659
私たちは別のを持っているノードの数

97
00:04:17.329 --> 00:04:22.170
それぞれの層Lの層もまた行きます

98
00:04:20.659 --> 00:04:30.450
使用します

99
00:04:22.170 --> 00:04:32.760
そう彼らLでアクティベーションを示すために

100
00:04:30.450 --> 00:04:35.550
私たちは繁殖のために後でその中に表示されます

101
00:04:32.760 --> 00:04:41.640
あなたにLを計算に終わります

102
00:04:35.550 --> 00:04:44.820
活性化Gは、おそらくZL MNに適用します

103
00:04:41.640 --> 00:04:50.430
活性化は、層Lとによってインデックスされます

104
00:04:44.820 --> 00:04:53.540
よく、その後、私たちはあなたを表すためにWLを使用します

105
00:04:50.430 --> 00:04:58.740
計算するための重みを知っています

106
00:04:53.540 --> 00:05:01.770
値ARLにおけるVLと同様にBLであります

107
00:04:58.740 --> 00:05:03.780
最終的にはちょうどラップするZLを計算するために使用

108
00:05:01.770 --> 00:05:07.080
表記上のアップ入力機能

109
00:05:03.780 --> 00:05:10.530
Xと呼ばれるが、Xもあるさ

110
00:05:07.080 --> 00:05:13.710
レイヤー0のアクティベーションので0に等しいです。

111
00:05:10.530 --> 00:05:17.550
Xおよび最終の活性化

112
00:05:13.710 --> 00:05:19.680
資本Lの層は、Yの帽子に等しいので、

113
00:05:17.550 --> 00:05:22.920
上付き文字の角括弧の資本Lであります

114
00:05:19.680 --> 00:05:24.500
予測された出力に等しいです

115
00:05:22.920 --> 00:05:26.910
ニューラルネットワークの予測yの帽子

116
00:05:24.500 --> 00:05:28.470
あなたは今何深いの神経を知っています

117
00:05:26.910 --> 00:05:30.480
ネットワークは、同様のように見えます

118
00:05:28.470 --> 00:05:32.190
我々は説明してにするために使用します記法

119
00:05:30.480 --> 00:05:33.930
コンピューティングや泥棒ネットワーク

120
00:05:32.190 --> 00:05:35.760
表記の多くを導入した場合、私は知っています

121
00:05:33.930 --> 00:05:38.160
このビデオではなく、あなたが今まで忘れてしまった場合

122
00:05:35.760 --> 00:05:40.860
一部の記号は、我々はまた、投稿した何を意味するのか

123
00:05:38.160 --> 00:05:43.080
もちろん、ウェブサイトの表記シート上

124
00:05:40.860 --> 00:05:44.460
彼らは見て使用することができますか記法ガイド

125
00:05:43.080 --> 00:05:46.950
これらの異なる記号が何を意味するのかアップ

126
00:05:44.460 --> 00:05:49.080
私は何前方を記述したいと混ぜます

127
00:05:46.950 --> 00:05:52.580
ネットワークの外観のこのタイプでの伝播

128
00:05:49.080 --> 00:05:52.580
以下のようなのは、次のビデオに行きましょう