WEBVTT

1
00:00:00.329 --> 00:00:05.700
基本的な技術的アイデアが遅れている場合

2
00:00:03.060 --> 00:00:07.470
あなたのネットワークの背後にある深い学習は

3
00:00:05.700 --> 00:00:09.870
なぜ彼らは何十年も前から存在していた

4
00:00:07.470 --> 00:00:12.090
このビデオでは今すぐ離陸しています

5
00:00:09.870 --> 00:00:14.130
いくつかの主要なドライバを見てみましょう

6
00:00:12.090 --> 00:00:16.170
深い学習の後ろに

7
00:00:14.130 --> 00:00:18.090
私はこれがあなたにスポットを助けるだろうと思う

8
00:00:16.170 --> 00:00:20.850
あなた自身の中で最高の機会

9
00:00:18.090 --> 00:00:22.439
これらを組織に適用する組織

10
00:00:20.850 --> 00:00:24.240
ここ数年、多くの人が

11
00:00:22.439 --> 00:00:26.820
Andrewになぜ深い学習があるのか​​尋ねました

12
00:00:24.240 --> 00:00:28.949
確かにうまくいきます。

13
00:00:26.820 --> 00:00:31.109
marsanの質問これは通常、

14
00:00:28.949 --> 00:00:33.210
私は彼らのために描く

15
00:00:31.109 --> 00:00:36.180
図を水平のどこにプロットするか

16
00:00:33.210 --> 00:00:39.270
私たちが持っているデータの量をプロットします

17
00:00:36.180 --> 00:00:42.570
タスクのために、縦に言ってみましょう

18
00:00:39.270 --> 00:00:44.430
私たちは上記のパフォーマンスをプロットします

19
00:00:42.570 --> 00:00:48.180
精度などの学習アルゴリズム

20
00:00:44.430 --> 00:00:51.960
スパム分類器または広告のクリック

21
00:00:48.180 --> 00:00:53.969
予測子または私たちの神経の正確さ

22
00:00:51.960 --> 00:00:56.399
ポジションを把握するためのネット

23
00:00:53.969 --> 00:00:58.440
自走車の他の呼びかけ

24
00:00:56.399 --> 00:01:00.270
パフォーマンスをプロットすると

25
00:00:58.440 --> 00:01:02.460
のような伝統的な学習アルゴリズム

26
00:01:00.270 --> 00:01:04.710
サポートベクターマシンまたはロジスティック

27
00:01:02.460 --> 00:01:07.619
量の関数としての回帰

28
00:01:04.710 --> 00:01:09.720
あなたは曲線を得るかもしれないデータの

29
00:01:07.619 --> 00:01:11.670
このように見えるのは、

30
00:01:09.720 --> 00:01:14.280
パフォーマンスはしばらく改善されます

31
00:01:11.670 --> 00:01:16.200
より多くのデータを追加しますが、しばらくしてから

32
00:01:14.280 --> 00:01:18.630
かなり知っているパフォーマンス

33
00:01:16.200 --> 00:01:21.180
プラトー右あなたの水平を仮定する

34
00:01:18.630 --> 00:01:25.320
あなたが知っていたことをよく知っています

35
00:01:21.180 --> 00:01:28.140
それは彼らが何をすべきかわからなかった

36
00:01:25.320 --> 00:01:30.689
データの量と私たちの

37
00:01:28.140 --> 00:01:32.850
過去10年間の社会はおそらく

38
00:01:30.689 --> 00:01:34.820
私たちが行った多くの問題

39
00:01:32.850 --> 00:01:38.610
比較的少量のデータを有する

40
00:01:34.820 --> 00:01:40.979
あなたはしばしばかなり大きいことを知っていることに

41
00:01:38.610 --> 00:01:43.979
データの量はこれのすべてでした

42
00:01:40.979 --> 00:01:46.979
社会のデジタル化のおかげで

43
00:01:43.979 --> 00:01:48.720
そんなに多くの人間活動が今どこにあるのか

44
00:01:46.979 --> 00:01:51.180
我々が非常に多くの時間を費やすデジタル領域

45
00:01:48.720 --> 00:01:54.320
モバイル上のウェブサイト上のコンピュータで

46
00:01:51.180 --> 00:01:57.960
デジタルデバイス上のアプリとアクティビティ

47
00:01:54.320 --> 00:02:00.360
データの作成と

48
00:01:57.960 --> 00:02:02.369
私たちのセルに内蔵された安価なカメラ

49
00:02:00.360 --> 00:02:05.909
携帯電話の加速度計のすべての種類

50
00:02:02.369 --> 00:02:07.890
物事のインターネットのセンサー

51
00:02:05.909 --> 00:02:11.129
もう1つだけ収集しています

52
00:02:07.890 --> 00:02:12.870
過去20年間でより多くのデータ

53
00:02:11.129 --> 00:02:13.560
多くのアプリケーションでは、

54
00:02:12.870 --> 00:02:16.319
累積する

55
00:02:13.560 --> 00:02:17.550
伝統的なものよりもはるかに多くのデータ

56
00:02:16.319 --> 00:02:20.520
学習アルゴリズムは

57
00:02:17.550 --> 00:02:22.560
効果的に

58
00:02:20.520 --> 00:02:26.310
新しいネットワークのリードは、あなたが

59
00:02:22.560 --> 00:02:28.470
小さな神経網を訓練して

60
00:02:26.310 --> 00:02:31.349
パフォーマンスはそのように見えるかもしれません

61
00:02:28.470 --> 00:02:34.590
あなたが幾分大きなインターネットを鍛えれば

62
00:02:31.349 --> 00:02:36.330
これは中規模のインターネットと呼ばれています

63
00:02:34.590 --> 00:02:39.900
少し上手く落ちる

64
00:02:36.330 --> 00:02:42.180
あなたが非常に大きな神経網を訓練するならば

65
00:02:39.900 --> 00:02:44.580
それはフォームであり、しばしば

66
00:02:42.180 --> 00:02:46.890
カップルがより良くなり、より良くなる

67
00:02:44.580 --> 00:02:49.410
あなたがヒットしたい場合は1つの観察です

68
00:02:46.890 --> 00:02:52.620
この非常に高いレベルのパフォーマンス

69
00:02:49.410 --> 00:02:54.420
最初に必要なものが2つ必要です

70
00:02:52.620 --> 00:02:57.360
十分に大きな神経を鍛えることができるようにする

71
00:02:54.420 --> 00:02:59.670
ネットワークを利用するには

72
00:02:57.360 --> 00:03:02.010
膨大な量のデータと2番目の

73
00:02:59.670 --> 00:03:05.430
あなたがするx軸上にここに出る必要があります

74
00:03:02.010 --> 00:03:07.799
多くのデータが必要です

75
00:03:05.430 --> 00:03:10.860
規模が深い学習を推進している

76
00:03:07.799 --> 00:03:12.900
進歩とスケールで私は両方の

77
00:03:10.860 --> 00:03:15.150
我々が必要とするニューラルネットワークのサイズ

78
00:03:12.900 --> 00:03:17.069
新しいネットワーク多くの隠されたユニット

79
00:03:15.150 --> 00:03:21.480
パラメータの多くは、多くの接続

80
00:03:17.069 --> 00:03:23.910
実際のデータの規模

81
00:03:21.480 --> 00:03:25.440
今日、最も信頼できる方法の1つです

82
00:03:23.910 --> 00:03:27.390
神経のパフォーマンスを向上させる

83
00:03:25.440 --> 00:03:29.940
ネットワークは、多くの場合、

84
00:03:27.390 --> 00:03:31.829
より大きなネットワークまたはそれ以上のデータを投げる

85
00:03:29.940 --> 00:03:33.359
それは一点までしか動かない

86
00:03:31.829 --> 00:03:35.640
最終的にはデータが不足しているため

87
00:03:33.359 --> 00:03:37.769
または結局あなたのネットワークはそうです

88
00:03:35.640 --> 00:03:40.200
大きな訓練には時間がかかりますが

89
00:03:37.769 --> 00:03:42.690
ちょうどスケールを改善して実際に撮影した

90
00:03:40.200 --> 00:03:45.810
私たちは学習の世界で長い道のりです

91
00:03:42.690 --> 00:03:48.060
この図をもう少し作成するために

92
00:03:45.810 --> 00:03:49.920
技術的に正確で、ほんの少し追加する

93
00:03:48.060 --> 00:03:53.040
私がデータの量を書いたより多くの事柄

94
00:03:49.920 --> 00:03:57.900
技術的にはx軸上の量です

95
00:03:53.040 --> 00:04:00.180
ラベルデータの

96
00:03:57.900 --> 00:04:03.630
私は両方のトレーニングの例を意味する

97
00:04:00.180 --> 00:04:05.910
入力XとラベルY Iは

98
00:04:03.630 --> 00:04:07.709
その表記法を少し紹介します

99
00:04:05.910 --> 00:04:10.769
このコースの後半で使用します

100
00:04:07.709 --> 00:04:12.540
小文字アルファベットを使用する

101
00:04:10.769 --> 00:04:13.739
私の訓練セットのサイズまたは

102
00:04:12.540 --> 00:04:15.690
トレーニングの例の数

103
00:04:13.739 --> 00:04:18.989
この小文字のMはそうです

104
00:04:15.690 --> 00:04:20.310
水平軸は他の詳細を

105
00:04:18.989 --> 00:04:23.340
このティガー

106
00:04:20.310 --> 00:04:26.970
より小さなトレーニングセットのこの体制では

107
00:04:23.340 --> 00:04:29.700
アルゴリズムの相対的な順序

108
00:04:26.970 --> 00:04:31.590
実際にはあまりよく定義されていません

109
00:04:29.700 --> 00:04:34.500
あなたは多くの訓練データを持っていない

110
00:04:31.590 --> 00:04:36.510
多くの場合あなたのスキルを手にして

111
00:04:34.500 --> 00:04:39.090
エンジニアリング機能は、

112
00:04:36.510 --> 00:04:41.910
監督のように

113
00:04:39.090 --> 00:04:44.070
SVMを訓練している人はもっとです

114
00:04:41.910 --> 00:04:46.320
エンジニアの機能と

115
00:04:44.070 --> 00:04:48.300
誰かが自分でも大きな訓練をしている

116
00:04:46.320 --> 00:04:50.730
この小さなトレーニングセットに含まれるかもしれない

117
00:04:48.300 --> 00:04:53.130
SEMがより良くできる体制

118
00:04:50.730 --> 00:04:55.020
あなたはこの地域で左に知っています

119
00:04:53.130 --> 00:04:57.090
図の相対的な順序

120
00:04:55.020 --> 00:04:59.550
遺伝子アルゴリズム間でうまくいかない

121
00:04:57.090 --> 00:05:01.919
定義され、パフォーマンスは大きく依存する

122
00:04:59.550 --> 00:05:03.389
エンジン機能のスキルをもっと

123
00:05:01.919 --> 00:05:05.970
その他の携帯端末の詳細

124
00:05:03.389 --> 00:05:08.850
アルゴリズムと、この中には

125
00:05:05.970 --> 00:05:12.000
大きなデータ体制の非常に大きなトレーニングセット

126
00:05:08.850 --> 00:05:14.669
右の非常に大きなM体制

127
00:05:12.000 --> 00:05:17.639
もっと一貫してRonettesを見る

128
00:05:14.669 --> 00:05:19.560
他のアプローチを支配する

129
00:05:17.639 --> 00:05:21.600
あなたの友人のいずれかがあなたになぜ尋ねるのか

130
00:05:19.560 --> 00:05:23.700
あなたが離陸を知っていると知られている私は

131
00:05:21.600 --> 00:05:26.729
この絵を描くことをお勧めします

132
00:05:23.700 --> 00:05:28.890
それらも同様に私は

133
00:05:26.729 --> 00:05:29.310
彼らの近代的な深みの黎明期

134
00:05:28.890 --> 00:05:32.070
学習

135
00:05:29.310 --> 00:05:34.919
スケールされたデータとスケール

136
00:05:32.070 --> 00:05:36.330
列車の能力だけを計算する

137
00:05:34.919 --> 00:05:39.479
非常に大きなディナーネットワーク

138
00:05:36.330 --> 00:05:41.850
私たちを可能にしたCPUまたはGPU

139
00:05:39.479 --> 00:05:43.590
プロをたくさん作る

140
00:05:41.850 --> 00:05:45.800
ますます特に最後に

141
00:05:43.590 --> 00:05:48.360
私たちは数年前に

142
00:05:45.800 --> 00:05:50.539
アルゴリズム革新だけでなく、私も

143
00:05:48.360 --> 00:05:53.700
それを過小評価したくない

144
00:05:50.539 --> 00:05:56.940
興味深いことに多くのアルゴリズム

145
00:05:53.700 --> 00:06:01.139
イノベーションは

146
00:05:56.940 --> 00:06:03.510
ニューラルネットワークをもっと速く動かせるようにする

147
00:06:01.139 --> 00:06:05.310
具体的な例としては、

148
00:06:03.510 --> 00:06:08.729
あなたのネットワークのブレークスルーは

149
00:06:05.310 --> 00:06:12.330
シグモイド関数から

150
00:06:08.729 --> 00:06:14.760
レーダー機能にこのように見える

151
00:06:12.330 --> 00:06:18.479
私たちは簡単に

152
00:06:14.760 --> 00:06:20.190
早いビデオあなたの

153
00:06:18.479 --> 00:06:22.260
1つの詳細を理解していない

154
00:06:20.190 --> 00:06:24.389
状態については心配しないでください。

155
00:06:22.260 --> 00:06:26.010
の問題の1つが判明した

156
00:06:24.389 --> 00:06:27.870
シグモイド関数と機械を使って

157
00:06:26.010 --> 00:06:29.520
学習は、これらの地域

158
00:06:27.870 --> 00:06:30.280
ここで関数の傾き

159
00:06:29.520 --> 00:06:32.920
〜する

160
00:06:30.280 --> 00:06:35.350
グラジエントはほぼゼロであるため、学習

161
00:06:32.920 --> 00:06:37.060
あなたが

162
00:06:35.350 --> 00:06:39.639
グラデーションディセントとグラデーションを実装する

163
00:06:37.060 --> 00:06:41.470
パラメータがゼロに変わるだけです

164
00:06:39.639 --> 00:06:44.740
ゆっくりと学習はとても遅いです

165
00:06:41.470 --> 00:06:46.450
一方、

166
00:06:44.740 --> 00:06:48.600
活性化機能は神経

167
00:06:46.450 --> 00:06:52.060
ネットワークと呼ばれるこの機能を使用する

168
00:06:48.600 --> 00:06:54.970
整流された線形の値関数

169
00:06:52.060 --> 00:06:57.070
ユニットのeluはグラジエントと等しい

170
00:06:54.970 --> 00:07:00.220
1つはすべての正の値の入力

171
00:06:57.070 --> 00:07:03.100
右のように、勾配ははるかに小さい

172
00:07:00.220 --> 00:07:04.750
徐々にゼロまで縮小する可能性があります

173
00:07:03.100 --> 00:07:07.300
ここの勾配はこの線の傾き

174
00:07:04.750 --> 00:07:09.520
左はゼロですが、それは分かります

175
00:07:07.300 --> 00:07:12.580
シグモイドに切り替えるだけで

176
00:07:09.520 --> 00:07:14.410
rayleigh関数に関数があります

177
00:07:12.580 --> 00:07:16.960
gradientと呼ばれるアルゴリズムを作った

178
00:07:14.410 --> 00:07:19.169
降下作業がはるかに速く、これは

179
00:07:16.960 --> 00:07:22.030
おそらく比較的単純な例

180
00:07:19.169 --> 00:07:23.860
アルゴリズムはベイジアンではなく、最終的には

181
00:07:22.030 --> 00:07:27.520
このアルゴリズム革新の影響

182
00:07:23.860 --> 00:07:29.080
それは本当に計算を望んでいたので、

183
00:07:27.520 --> 00:07:31.240
レジメンのような例がかなりたくさんあります

184
00:07:29.080 --> 00:07:33.340
アルゴリズムの変更箇所

185
00:07:31.240 --> 00:07:35.140
そのコードを多く実行することができるからです

186
00:07:33.340 --> 00:07:37.479
より速く、これは私達が

187
00:07:35.140 --> 00:07:39.520
より大きいニューラルネットワークまたはそれを行うには

188
00:07:37.479 --> 00:07:42.250
理由またはマルチクライアント

189
00:07:39.520 --> 00:07:45.810
大規模なネットワークがすべてのデータをローミング

190
00:07:42.250 --> 00:07:48.610
高速計算が他の理由である

191
00:07:45.810 --> 00:07:51.070
重要なことは、

192
00:07:48.610 --> 00:07:53.710
あなたのネットワークをトレーニングするプロセスです

193
00:07:51.070 --> 00:07:56.350
非常に直感的なアイデアが多い

194
00:07:53.710 --> 00:07:58.020
ニューラルネットワークアーキテクチャのために

195
00:07:56.350 --> 00:08:01.060
あなたのアイデアとコードを実装する

196
00:07:58.020 --> 00:08:02.830
あなたのアイデアを実装して

197
00:08:01.060 --> 00:08:05.050
どのようにうまくいくかを示す実験

198
00:08:02.830 --> 00:08:07.510
あなたのニューラルネットワークは行います。

199
00:08:05.050 --> 00:08:10.030
それを見て、あなたは戻って

200
00:08:07.510 --> 00:08:12.930
あなたの新しいネットワークの詳細

201
00:08:10.030 --> 00:08:15.880
このサークルを何度も繰り返して

202
00:08:12.930 --> 00:08:18.550
あなたの新しいネットワークが長い時間がかかるとき

203
00:08:15.880 --> 00:08:21.400
訓練するにはちょうど時間がかかる

204
00:08:18.550 --> 00:08:24.039
このサイクルの周りには巨大なものがあります

205
00:08:21.400 --> 00:08:26.740
あなたの生産性の違い

206
00:08:24.039 --> 00:08:29.560
あなたができるときに効果的なニューラルネットワーク

207
00:08:26.740 --> 00:08:34.169
アイデアを持って試してみてください

2208
00:08:29.560 --> 00:08:36.370
10分またはおそらくは1日にammosで

209
00:08:34.169 --> 00:08:39.490
あなたがあなたの神経を鍛えなければならない場合

210
00:08:36.370 --> 00:08:40.590
時には1ヶ月間のネットワーク

211
00:08:39.490 --> 00:08:42.570
起こった

212
00:08:40.590 --> 00:08:44.670
あなたが知っている結果を得るので

213
00:08:42.570 --> 00:08:47.250
10分で、あるいは1日に

214
00:08:44.670 --> 00:08:49.170
もっと多くのアイデアを試して

215
00:08:47.250 --> 00:08:50.610
あなたの中で発見する可能性が非常に高い

216
00:08:49.170 --> 00:08:53.720
ネットワークとそれはあなたのためにうまく動作します

217
00:08:50.610 --> 00:08:57.900
アプリケーションと非常に高速な計算

218
00:08:53.720 --> 00:08:59.730
スピードの面で本当に助けになりました

219
00:08:57.900 --> 00:09:02.610
あなたが得ることができる率を上げる

220
00:08:59.730 --> 00:09:05.400
実験結果が戻ってこれが

221
00:09:02.610 --> 00:09:07.550
実際に両方の開業医を助けました

222
00:09:05.400 --> 00:09:10.650
ニューロネットワークと研究者

223
00:09:07.550 --> 00:09:13.320
作業と深い学習は多くを反復する

224
00:09:10.650 --> 00:09:16.589
もっと早くあなたのアイディアを改善してください

225
00:09:13.320 --> 00:09:18.570
より速く、これもすべて

226
00:09:16.589 --> 00:09:21.029
完全な深い学習への巨大な恩恵

227
00:09:18.570 --> 00:09:23.370
されている研究コミュニティ

228
00:09:21.029 --> 00:09:25.620
信じられないほどあなたが発明して知っていると

229
00:09:23.370 --> 00:09:28.920
新しいアルゴリズムとノンストップ

230
00:09:25.620 --> 00:09:30.990
その前進が進んでいるので

231
00:09:28.920 --> 00:09:33.570
深部の上昇に力を与える力の

232
00:09:30.990 --> 00:09:36.000
良いニュースはこれらのことです

233
00:09:33.570 --> 00:09:38.490
力はまだ強力に働いている

234
00:09:36.000 --> 00:09:41.130
深い学習をより良くするTech Data

235
00:09:38.490 --> 00:09:43.800
社会はまだもう一人を投げ掛けている

236
00:09:41.130 --> 00:09:45.660
デジタルデータ、または

237
00:09:43.800 --> 00:09:48.300
のような特殊なハードウェアの登場

238
00:09:45.660 --> 00:09:50.940
GPUとより高速なネットワーク

239
00:09:48.300 --> 00:09:53.250
ハードウェア私は実際にかなり自信があります

240
00:09:50.940 --> 00:09:55.140
非常に大きな神経を行う私たちの能力

241
00:09:53.250 --> 00:09:57.320
ネットワークまたは計算ポイント

242
00:09:55.140 --> 00:10:00.360
見通しが良くなる

243
00:09:57.320 --> 00:10:02.880
アルゴリズムを相対的に学習する

244
00:10:00.360 --> 00:10:05.070
継続的に研究コミュニティ

245
00:10:02.880 --> 00:10:07.680
革新的な

246
00:10:05.070 --> 00:10:09.839
アルゴリズムは正にこのためにI

247
00:10:07.680 --> 00:10:11.370
私たちは楽観的な答えができると思う

248
00:10:09.839 --> 00:10:13.650
楽観的な深い学習は

249
00:10:11.370 --> 00:10:14.120
何年も前から良くなっています

250
00:10:13.650 --> 00:10:17.100
来る

251
00:10:14.120 --> 00:10:18.540
最後のビデオに進む

252
00:10:17.100 --> 00:10:20.280
私たちが少し話をするセクション

253
00:10:18.540 --> 00:10:22.610
あなたがこのことから学ぶことについてもっと知る

254
00:10:20.280 --> 00:10:22.610
コース
